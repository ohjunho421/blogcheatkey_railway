# d:\BlogCheatKey\blog_cheatkey_v2\blog_cheatkey\backend\content\services\generator.py
import re
import json
import logging
import time
import traceback
from urllib.parse import urlparse
import anthropic
from konlpy.tag import Okt
from django.conf import settings
from django.db import transaction
from anthropic import Anthropic, RateLimitError
from backend.research.models import ResearchSource, StatisticData
from backend.key_word.models import Keyword, Subtopic
from backend.content.models import BlogContent, MorphemeAnalysis
from backend.accounts.models import User
from .substitution_generator import SubstitutionGenerator
from .morpheme_analyzer import MorphemeAnalyzer 

logger = logging.getLogger(__name__)


class ContentGenerator:
    """
    Claude APIë¥¼ ì‚¬ìš©í•œ ë¸”ë¡œê·¸ ì½˜í…ì¸  ìƒì„± ì„œë¹„ìŠ¤
    - ìƒì„±ê³¼ ë™ì‹œì— ìµœì í™” ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì½˜í…ì¸  ìƒì„±
    """
    
    def __init__(self):
        self.anthropic_api_key = settings.ANTHROPIC_API_KEY
        self.model = "claude-sonnet-4-20250514" # Model updated
        self.client = Anthropic(api_key=self.anthropic_api_key)
        self.okt = Okt()
        self.max_retries = 3 # API í˜¸ì¶œ ì¬ì‹œë„ íšŸìˆ˜
        self.retry_delay = 5 # ì¬ì‹œë„ ê°„ê²© (ì´ˆ)
        self.substitution_generator = SubstitutionGenerator()
        self.morpheme_analyzer = MorphemeAnalyzer() # Instance of the new MorphemeAnalyzer
    
    def generate_content(self, keyword_id, user_id, target_audience=None, business_info=None, custom_morphemes=None, subtopics_list=None):
        """
        í‚¤ì›Œë“œ ê¸°ë°˜ ë¸”ë¡œê·¸ ì½˜í…ì¸  ìƒì„± (ìµœì í™” ì¡°ê±´ ì¶©ì¡±)
        
        Args:
            keyword_id (int): í‚¤ì›Œë“œ ID
            user_id (int): ì‚¬ìš©ì ID
            target_audience (dict): íƒ€ê²Ÿ ë…ì ì •ë³´
            business_info (dict): ì‚¬ì—…ì ì •ë³´
            custom_morphemes (list): ì‚¬ìš©ì ì§€ì • í˜•íƒœì†Œ ëª©ë¡
            subtopics_list (list): ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬ëœ ì†Œì œëª© ëª©ë¡ (ê¸°ë³¸ê°’ None)
            
        Returns:
            tuple: (ìƒì„±ëœ BlogContent ê°ì²´ì˜ ID, ì°¸ê³  ìë£Œ ëª©ë¡) ë˜ëŠ” ì‹¤íŒ¨ ì‹œ (None, [])
        """
        for attempt in range(self.max_retries):
            try:
                keyword_obj = Keyword.objects.get(id=keyword_id)
                keyword_text = keyword_obj.keyword
                user = User.objects.get(id=user_id)
                
                current_subtopics = subtopics_list
                if current_subtopics is None:
                    current_subtopics = list(keyword_obj.subtopics.order_by('order').values_list('title', flat=True))
                
                # ê° ì†ŒìŠ¤ ìœ í˜•ë³„ë¡œ ìµœì‹  5ê°œì˜ IDë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
                news_ids = list(ResearchSource.objects.filter(keyword=keyword_obj, source_type='news').order_by('-created_at').values_list('id', flat=True)[:5])
                academic_ids = list(ResearchSource.objects.filter(keyword=keyword_obj, source_type='academic').order_by('-created_at').values_list('id', flat=True)[:5])
                general_ids = list(ResearchSource.objects.filter(keyword=keyword_obj, source_type='general').order_by('-created_at').values_list('id', flat=True)[:5])
                
                # ì¤‘ë³µì„ ì œê±°í•˜ê³  ëª¨ë“  IDë¥¼ í•©ì¹©ë‹ˆë‹¤.
                all_source_ids = list(set(news_ids + academic_ids + general_ids))
                
                # ìµœì¢…ì ìœ¼ë¡œ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •ë ¬ëœ ì¿¼ë¦¬ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤.
                sources = ResearchSource.objects.filter(id__in=all_source_ids).order_by('-published_date', '-created_at')
                statistics = StatisticData.objects.filter(source__keyword=keyword_obj)
                
                existing_content = BlogContent.objects.filter(
                    keyword=keyword_obj, 
                    user=user, 
                    title__contains="(ìƒì„± ì¤‘...)"
                ).order_by('-created_at').first()
                
                data_for_prompt = {
                    "keyword": keyword_text,
                    "subtopics": current_subtopics,
                    "target_audience": target_audience or {
                        "primary": keyword_obj.main_intent or "ì¼ë°˜ ì‚¬ìš©ì",
                        "pain_points": keyword_obj.pain_points or ["ì •ë³´ ë¶€ì¡±"]
                    },
                    "business_info": business_info or {
                        "name": user.username,
                        "expertise": user.profile.expertise if hasattr(user, 'profile') and hasattr(user.profile, 'expertise') else "ê´€ë ¨ ë¶„ì•¼ ì „ë¬¸ê°€"
                    },
                    "custom_morphemes": custom_morphemes, 
                }
                
                data_for_prompt = self._prepare_prompt_data(keyword_obj, current_subtopics, sources, statistics, user, custom_morphemes)
                
                logger.info(f"ì½˜í…ì¸  ìƒì„± API í˜¸ì¶œ ì‹œì‘ (ì‹œë„ {attempt+1}/{self.max_retries}): í‚¤ì›Œë“œ={keyword_text}, ì‚¬ìš©ì={user.username}")
                logger.info(f"ì½˜í…ì¸  ìƒì„±ì— ì‚¬ìš©ë˜ëŠ” ì†Œì œëª©: {current_subtopics}")

                prompt = self._create_optimized_content_prompt(data_for_prompt)
                
                response = self.client.messages.create(
                    model=self.model,
                    max_tokens=4096,
                    temperature=0.7,
                    messages=[{"role": "user", "content": prompt}]
                )
                
                logger.info("ì½˜í…ì¸  ìƒì„± API í˜¸ì¶œ ì™„ë£Œ")
                
                generated_content_text = response.content[0].text
                
                initial_analysis = self.morpheme_analyzer.analyze(generated_content_text, keyword_text, custom_morphemes)
                
                final_content_to_save = generated_content_text
                final_analysis_for_db = initial_analysis

                if not initial_analysis['is_fully_optimized']:
                    logger.info("1ì°¨ ìƒì„± ì½˜í…ì¸  ìµœì í™” í•„ìš”. ì¶”ê°€ ìµœì í™” ì‹œë„.")
                    logger.info(f"1ì°¨ ê²€ì¦ ê²°ê³¼: ê¸€ììˆ˜={initial_analysis['char_count']} (ìœ íš¨: {initial_analysis['is_valid_char_count']}), ëª©í‘œí˜•íƒœì†Œ ìœ íš¨={initial_analysis['is_valid_morphemes']}")
                    
                    optimization_prompt = self._create_verification_optimization_prompt(
                        generated_content_text, 
                        keyword_text, 
                        custom_morphemes,
                        initial_analysis
                    )
                    
                    optimization_response = self.client.messages.create(
                        model=self.model,
                        max_tokens=4096,
                        temperature=0.5,
                        messages=[{"role": "user", "content": optimization_prompt}]
                    )
                    
                    optimized_content_after_verify_prompt = optimization_response.content[0].text
                    analysis_after_verify_prompt = self.morpheme_analyzer.analyze(optimized_content_after_verify_prompt, keyword_text, custom_morphemes)
                    
                    logger.info(f"ì¶”ê°€ ìµœì í™” ì‹œë„ í›„ ê²°ê³¼: ê¸€ììˆ˜={analysis_after_verify_prompt['char_count']}, ëª©í‘œí˜•íƒœì†Œ ìœ íš¨={analysis_after_verify_prompt['is_valid_morphemes']}")

                    if self.morpheme_analyzer.is_better_optimization(analysis_after_verify_prompt, initial_analysis):
                        final_content_to_save = optimized_content_after_verify_prompt
                        final_analysis_for_db = analysis_after_verify_prompt
                        logger.info("ì¶”ê°€ ìµœì í™”ëœ ì½˜í…ì¸  ì‚¬ìš©: ë” ë‚˜ì€ ê²°ê³¼")
                    else:
                        logger.info("1ì°¨ ìƒì„± ì½˜í…ì¸  ì‚¬ìš©: ì¶”ê°€ ìµœì í™” í›„ ê°œì„ ë˜ì§€ ì•ŠìŒ")
                
                content_with_references = self._add_references(final_content_to_save, data_for_prompt['research_data'])
                mobile_formatted_content = self._format_for_mobile(content_with_references)
                references_list = self._extract_references(content_with_references)
                
                if existing_content:
                    existing_content.delete()
                
                source_data = [{'title': s.title, 'url': s.url} for s in sources if s.url]

                blog_content = BlogContent.objects.create(
                    user=user,
                    keyword=keyword_obj,
                    title=f"{keyword_text} ì™„ë²½ ê°€ì´ë“œ", 
                    content=content_with_references,
                    mobile_formatted_content=mobile_formatted_content,
                    references=references_list,
                    char_count=final_analysis_for_db['char_count'],
                    is_optimized=final_analysis_for_db['is_fully_optimized'] 
                )
                
                logger.info("í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹œì‘")
                if 'morpheme_analysis' in final_analysis_for_db and 'counts' in final_analysis_for_db['morpheme_analysis']:
                    for morpheme, info in final_analysis_for_db['morpheme_analysis']['counts'].items():
                        MorphemeAnalysis.objects.create(
                            content=blog_content,
                            morpheme=morpheme,
                            count=info.get('count', 0),
                            is_valid=info.get('is_valid', False),
                            morpheme_type=info.get('type', 'unknown') # Save morpheme type
                        )
                
                logger.info(f"ì½˜í…ì¸  ìƒì„± ì™„ë£Œ: ID={blog_content.id}")
                return blog_content.id, source_data
                    
            except RateLimitError as e:
                logger.warning(f"Anthropic API ê³¼ë¶€í•˜ (ì‹œë„ {attempt+1}/{self.max_retries}). ì˜¤ë¥˜: {e}")
                if attempt >= self.max_retries - 1:
                    logger.error("ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. API ê³¼ë¶€í•˜ê°€ ì§€ì†ë©ë‹ˆë‹¤.")
                    if existing_content:
                        existing_content.title = f"{keyword_text} (ìƒì„± ì‹¤íŒ¨)"
                        existing_content.content = f"ì½˜í…ì¸  ìƒì„± ì¤‘ ìµœì¢… ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
                        existing_content.save()
                    return None, []
                
                # Exponential backoff: 1s, 2s, 4s, ... + random jitter
                wait_time = (2 ** attempt) + random.random()
                logger.info(f"{wait_time:.2f}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
                time.sleep(wait_time)

            except anthropic.APIError as e:
                logger.error(f"ì½˜í…ì¸  ìƒì„± ì¤‘ API ì˜¤ë¥˜ ë°œìƒ (ì‹œë„ {attempt+1}/{self.max_retries}): {e}")
                traceback.print_exc()
                if attempt >= self.max_retries - 1:
                    logger.error("ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. API ì˜¤ë¥˜ë¡œ ì½˜í…ì¸  ìƒì„± ì‹¤íŒ¨.")
                    if existing_content:
                        existing_content.title = f"{keyword_text} (ìƒì„± ì‹¤íŒ¨)"
                        existing_content.content = f"ì½˜í…ì¸  ìƒì„± ì¤‘ ìµœì¢… ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
                        existing_content.save()
                    return None, []
                time.sleep(self.retry_delay) # Fixed delay for other API errors

            except Exception as e:
                logger.error(f"ì½˜í…ì¸  ìƒì„± ì‹¤íŒ¨ (í‚¤ì›Œë“œ ID: {keyword_id}): {e}")
                traceback.print_exc()
                if existing_content:
                    existing_content.title = f"{keyword_text} (ìƒì„± ì‹¤íŒ¨)"
                    existing_content.content = f"ì½˜í…ì¸  ìƒì„± ì¤‘ ìµœì¢… ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
                    existing_content.save()
                return None, [] # For unexpected errors, fail fast
                    
    def _prepare_prompt_data(self, keyword_obj, subtopics, sources, statistics, user, custom_morphemes):
        research_data = self._format_research_data(sources, statistics)
        
        return {
            "keyword": keyword_obj.keyword,
            "subtopics": subtopics,
            "target_audience": {
                "primary": keyword_obj.main_intent or "ì¼ë°˜ ì‚¬ìš©ì",
                "pain_points": keyword_obj.pain_points or ["ì •ë³´ ë¶€ì¡±"]
            },
            "business_info": {
                "name": user.username,
                "expertise": user.profile.expertise if hasattr(user, 'profile') and hasattr(user.profile, 'expertise') else "ê´€ë ¨ ë¶„ì•¼ ì „ë¬¸ê°€"
            },
            "custom_morphemes": custom_morphemes, 
            "research_data": research_data
        }

    def _format_research_data(self, sources, statistics):
        research_data = {'news': [], 'academic': [], 'general': [], 'statistics': []}
        
        # ResearchSource ë°ì´í„°ë¥¼ ì¢…ë¥˜ë³„ë¡œ ë¶„ë¥˜
        for source in sources.order_by('-published_date')[:15]: # ê° ì¢…ë¥˜ë³„ë¡œ 5ê°œì”© ê°€ì ¸ì˜¤ê¸° ìœ„í•´ ë„‰ë„‰íˆ 15ê°œë¥¼ ê°€ì ¸ì˜´
            if source.source_type in research_data and len(research_data[source.source_type]) < 5:
                research_data[source.source_type].append({
                    'title': source.title, 'url': source.url, 'snippet': source.snippet,
                    'date': source.published_date.isoformat() if source.published_date else '',
                    'source': source.author or urlparse(source.url).netloc
                })

        # StatisticData ì²˜ë¦¬
        for stat in statistics.order_by('-source__published_date')[:5]:
            research_data['statistics'].append({
                'value': stat.value, 'context': stat.context, 'pattern_type': stat.pattern_type,
                'source_url': stat.source.url, 'source_title': stat.source.title,
                'source': stat.source.author or urlparse(stat.source.url).netloc,
                'date': stat.source.published_date.isoformat() if stat.source.published_date else ''
            })
        return research_data

    def _create_optimized_content_prompt(self, data):
        keyword = data["keyword"]
        subtopics = data["subtopics"]
        target_audience = data["target_audience"]
        business_info = data["business_info"]
        custom_morphemes = data.get("custom_morphemes", [])
        research_data = data["research_data"]

        # Use MorphemeAnalyzer to get categorized morphemes and their target ranges
        dummy_analysis = self.morpheme_analyzer.analyze("", keyword, custom_morphemes)
        base_morphemes = dummy_analysis['morpheme_analysis']['target_morphemes']['base']
        compound_morphemes = dummy_analysis['morpheme_analysis']['target_morphemes']['compound']

        target_min_base_morph = self.morpheme_analyzer.target_min_base_count
        target_max_base_morph = self.morpheme_analyzer.target_max_base_count
        target_min_compound_morph = self.morpheme_analyzer.target_min_compound_count
        target_max_compound_morph = self.morpheme_analyzer.target_max_compound_count

        target_min_chars = self.morpheme_analyzer.target_min_chars
        target_max_chars = self.morpheme_analyzer.target_max_chars

        keyword_instruction_parts = []
        
        # Instructions for base morphemes
        if base_morphemes:
            keyword_instruction_parts.append(f"- í•µì‹¬ ê¸°ë³¸ í˜•íƒœì†Œ ({', '.join(base_morphemes)}): ê°ê° {target_min_base_morph}-{target_max_base_morph}íšŒ ì´ë‚´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì‚¬ìš© (ì˜ˆ: 'ì—”ì§„ì˜¤ì¼ì¢…ë¥˜'ì—ì„œ 'ì—”ì§„', 'ì˜¤ì¼', 'ì¢…ë¥˜' ê°ê° ì¹´ìš´íŠ¸)")

        # Instructions for compound morphemes/phrases
        if compound_morphemes:
            keyword_instruction_parts.append(f"- ë³µí•© í‚¤ì›Œë“œ ë° êµ¬ë¬¸ ({', '.join(compound_morphemes)}): ê°ê° {target_min_compound_morph}-{target_max_compound_morph}íšŒ ì´ë‚´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì‚¬ìš© (ì˜ˆ: 'ì—”ì§„ì˜¤ì¼', 'ì—”ì§„ì˜¤ì¼ì¢…ë¥˜')")
        
        keyword_instruction = "\n".join(keyword_instruction_parts)
        
        research_text = ""
        target_audience = data.get('target_audience', {})
        business_info = data.get('business_info', {})
        research_data_dict = data.get('research_data', {})

        if isinstance(research_data_dict, dict):
            news = research_data_dict.get('news', [])[:2]
            academic = research_data_dict.get('academic', [])[:2]
            general = research_data_dict.get('general', [])[:2]
            
            if news:
                research_text += "ğŸ“° ë‰´ìŠ¤ ìë£Œ:\n"
                for item in news: research_text += f"- {item.get('title', '')} ({item.get('source', '')}, {item.get('date','')}): {item.get('snippet', '')}\n"
            if academic:
                research_text += "\nğŸ“š í•™ìˆ  ìë£Œ:\n"
                for item in academic: research_text += f"- {item.get('title', '')} ({item.get('source', '')}, {item.get('date','')}): {item.get('snippet', '')}\n"
            if general:
                research_text += "\nğŸ” ì¼ë°˜ ìë£Œ:\n"
                for item in general: research_text += f"- {item.get('title', '')} ({item.get('source', '')}, {item.get('date','')}): {item.get('snippet', '')}\n"

        statistics_text = ""
        if isinstance(research_data_dict.get('statistics'), list) and research_data_dict.get('statistics'):
            statistics_text = "\nğŸ’¡ í™œìš© ê°€ëŠ¥í•œ í†µê³„ ìë£Œ (ìµœì†Œ 1ê°œ ì´ìƒ ë³¸ë¬¸ì— ìì—°ìŠ¤ëŸ½ê²Œ ì¸ìš©):\n"
            for stat in research_data_dict['statistics'][:3]:
                date_info = f" ({stat.get('date', '')[:4]}ë…„)" if stat.get('date') and len(stat.get('date')) >=4 else ""
                statistics_text += f"- {stat.get('context', '')}{date_info} (ì¶œì²˜: {stat.get('source_title', stat.get('source','ì•Œ ìˆ˜ ì—†ìŒ'))})\n"
        else:
            statistics_text = "\n(í™œìš© ê°€ëŠ¥í•œ íŠ¹ì • í†µê³„ ìë£Œê°€ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ê²½í–¥ì´ë‚˜ ì¤‘ìš”ì„±ì„ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.)\n"


        optimization_requirements = f"""
        âš ï¸ ì¤‘ìš”: ë‹¤ìŒ ìµœì í™” ì¡°ê±´ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.

        1. ê¸€ììˆ˜ ì¡°ê±´: ì •í™•íˆ {target_min_chars}-{target_max_chars}ì (ê³µë°± ì œì™¸, ì°¸ê³ ìë£Œ ì„¹ì…˜ ì œì™¸)
        - ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ìœ ì§€í•˜ê±°ë‚˜ í•„ìš”ì‹œ í™•ì¥í•˜ì—¬ ì´ ë²”ìœ„ì— ë§ì¶”ê¸°

        2. í‚¤ì›Œë“œ ë° ì£¼ìš” í˜•íƒœì†Œ ì¶œí˜„ íšŸìˆ˜ ì¡°ê±´:
        {keyword_instruction}
        - ì¤‘ìš”: Ctrl+Fë¡œ ê²€ìƒ‰í–ˆì„ ë•Œ ìœ„ì— ì–¸ê¸‰ëœ ëª¨ë“  í‚¤ì›Œë“œì™€ í˜•íƒœì†Œê°€ ê°ê° ì§€ì •ëœ ë²”ìœ„ ë‚´ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤!

        3. í‚¤ì›Œë“œ ìµœì í™” ë°©ë²•:
        - ì§€ì‹œì–´ í™œìš©: "{keyword}ëŠ”" â†’ "ì´ê²ƒì€" ë“±
        - ìì—°ìŠ¤ëŸ¬ìš´ ìƒëµ: ë¬¸ë§¥ìƒ ì´í•´ ê°€ëŠ¥í•œ ê²½ìš° ìƒëµ
        - ë™ì˜ì–´/ìœ ì‚¬ì–´ ëŒ€ì²´: ê³¼ë‹¤ ì‚¬ìš©ëœ ë‹¨ì–´ë¥¼ ì ì ˆí•œ ë™ì˜ì–´ë¡œ ëŒ€ì²´ (ë‹¨, ëª©í‘œ í˜•íƒœì†ŒëŠ” ìœ ì§€)

        âœ“ ìµœì¢… ê²€ì¦: ìƒì„± ì™„ë£Œ í›„, ìœ„ì— ì–¸ê¸‰ëœ ëª¨ë“  ëª©í‘œ í‚¤ì›Œë“œ/í˜•íƒœì†Œê°€ **ê°ê°** ì§€ì •ëœ ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€, ê¸€ììˆ˜ê°€ ë§ëŠ”ì§€ **ë°˜ë“œì‹œ** ì¬í™•ì¸í•˜ì„¸ìš”. **ìµœëŒ€ íšŸìˆ˜ë¥¼ ë‹¨ 1íšŒë¼ë„ ì´ˆê³¼í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤. ì°¨ë¼ë¦¬ ìµœì†Œ íšŸìˆ˜ë³´ë‹¤ ì•½ê°„ ë¶€ì¡±í•œ ê²ƒì´ ë‚«ìŠµë‹ˆë‹¤.** ì´ ê·œì¹™ì€ ì ˆëŒ€ì ì…ë‹ˆë‹¤.
        """
        logger.info(f"í”„ë¡¬í”„íŠ¸ì— ì „ë‹¬ë˜ëŠ” ì†Œì œëª©: {data.get('subtopics', [])}")
        subtopics_for_prompt = data.get('subtopics', [])
        subtopic_lines = ""
        if subtopics_for_prompt:
            for i, st_title in enumerate(subtopics_for_prompt):
                subtopic_lines += f"        ### {st_title}\n"
        else:
            subtopic_lines = "        (ì†Œì œëª© ì—†ì´ ììœ ë¡­ê²Œ ë³¸ë¡  êµ¬ì„±)\n"


        prompt = f"""
        ë‹¹ì‹ ì€ {data.get('target_audience', {}).get('persona', 'ì „ë¬¸ ë¸”ë¡œê·¸ ì‘ê°€')}ì…ë‹ˆë‹¤. ë‹¤ìŒ ì§€ì¹¨ì— ë”°ë¼ '{keyword}' í‚¤ì›Œë“œì— ëŒ€í•œ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.

        ## ìµœì¢… ëª©í‘œ: ë…ìê°€ ê¸€ì„ ëê¹Œì§€ ì½ê³ , ì œì‹œëœ í•´ê²°ì±…ì— ë§Œì¡±í•˜ë©°, {data.get('business_info', {}).get('name', 'ìš°ë¦¬ íšŒì‚¬')}ë¥¼ ì‹ ë¢°í•˜ê²Œ ë§Œë“œëŠ” ê²ƒ

        ---

        ### **ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ ì‘ì„± í•„ìˆ˜ ì§€ì¹¨**

        1.  **í˜ë¥´ì†Œë‚˜ ë° íƒ€ê²Ÿ ë…ì:**
            -   **ì‘ì„±ì í˜ë¥´ì†Œë‚˜:** {data.get('target_audience', {}).get('persona', 'í•´ë‹¹ ë¶„ì•¼ì˜ ê¹Šì´ ìˆëŠ” ì „ë¬¸ê°€')}
            -   **íƒ€ê²Ÿ ë…ì:** {data.get('target_audience', {}).get('description', 'ì´ˆë³´ìë¶€í„° ì „ë¬¸ê°€ê¹Œì§€ ëª¨ë‘')}
            -   **ê¸€ì˜ ëª©ì :** {data.get('target_audience', {}).get('goal', 'ì •ë³´ ì œê³µ ë° ë¬¸ì œ í•´ê²°')}
            -   **ì–´ì¡°ì™€ ìŠ¤íƒ€ì¼:** {data.get('target_audience', {}).get('tone_and_style', 'ì „ë¬¸ì ì´ê³  ì‹ ë¢°ê° ìˆì§€ë§Œ, ì´í•´í•˜ê¸° ì‰¬ìš´ ì–´ì¡°')}

        2.  **[í•„ìˆ˜] ì„œë¡  ì‘ì„± ê°€ì´ë“œ (ì „ë¬¸ì„± ì–´í•„, ê³µê°, ê°•ë ¥í•œ ìœ ë„):**
            -   **ê³µê° í˜•ì„±:** ë…ìê°€ '{keyword}' ë¬¸ì œë¡œ ê²ªëŠ” 'êµ¬ì²´ì ì¸ ë¶ˆí¸í•¨'ê³¼ 'ë‹µë‹µí•œ ê°ì •'ì„ ì •í™•íˆ ì§šì–´ë‚´ë©° ê¹Šì€ ê³µê°ëŒ€ë¥¼ í˜•ì„±í•˜ì„¸ìš”. (ì˜ˆ: "í˜¹ì‹œ '{keyword}' ë¬¸ì œ ë•Œë¬¸ì— ë°¤ì  ì„¤ì¹˜ê³  ê³„ì‹ ê°€ìš”? ìˆ˜ë§ì€ ì •ë³´ë¥¼ ì°¾ì•„ë´¤ì§€ë§Œ, ê²°êµ­ ì‹œê°„ë§Œ ë‚­ë¹„í•œ ê²ƒ ê°™ì•„ í—ˆíƒˆí•˜ì‹ ê°€ìš”?")
            -   **ì „ë¬¸ì„± ì–´í•„ ë° ì‹ ë¢° êµ¬ì¶•:** "{data.get('business_info', {}).get('name', 'ìš°ë¦¬ íšŒì‚¬')}ëŠ” ì´ ë¶„ì•¼ì˜ ì „ë¬¸ê°€ë¡œì„œ ìˆ˜ë§ì€ ê³ ê°ë“¤ì˜ ë¬¸ì œë¥¼ í•´ê²°í•´ì™”ìŠµë‹ˆë‹¤. ê·¸ ê²½í—˜ê³¼ ë…¸í•˜ìš°ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì—¬ëŸ¬ë¶„ì˜ ì‹œê°„ì„ ì•„ê»´ì¤„ ê°€ì¥ íš¨ê³¼ì ì¸ ë°©ë²•ë§Œì„ ì•Œë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤." ì™€ ê°™ì´ ì €í¬ì˜ ì „ë¬¸ì„±ì„ ë“œëŸ¬ë‚´ ë…ìê°€ ê¸€ì„ ì‹ ë¢°í•˜ê²Œ ë§Œë“œì„¸ìš”.
            -   **í•´ê²°ì±… ì•½ì†:** ì´ ê¸€ì´ ë‹¨ìˆœ ì •ë³´ ë‚˜ì—´ì´ ì•„ë‹Œ, ë¬¸ì œë¥¼ 'í•´ê²°'í•  'ê²€ì¦ëœ ë°©ë²•'ê³¼ 'ì‹¤ìš©ì ì¸ íŒ'ì„ ì œê³µí•œë‹¤ëŠ” ì ì„ ëª…í™•íˆ ì•½ì†í•˜ì„¸ìš”.
            -   **ë…ì„œ ìœ ë„:** "ì´ ê¸€ì„ ë‹¨ 5ë¶„ë§Œ íˆ¬ìí•´ì„œ ëê¹Œì§€ ì½ìœ¼ì‹ ë‹¤ë©´, ë” ì´ìƒ í—¤ë§¤ì§€ ì•Šê³  ë¬¸ì œë¥¼ í•´ê²°í•  ëª…í™•í•œ ì²­ì‚¬ì§„ì„ ì–»ê²Œ ë  ê²ƒì…ë‹ˆë‹¤." ì™€ ê°™ì´, ê¸€ì„ ë†“ì¹˜ë©´ ì†í•´ë¼ëŠ” ì¸ì‹ì„ ì£¼ì–´ ëê¹Œì§€ ì½ë„ë¡ ê°•ë ¥í•˜ê²Œ ìœ ë„í•˜ì„¸ìš”.

        3.  **ë³¸ë¬¸ ì‘ì„± ê°€ì´ë“œ:**
            -   ì œê³µëœ ì†Œì œëª©(`subtopics`)ì„ ëª¨ë‘ ì‚¬ìš©í•˜ì—¬ ë³¸ë¬¸ì„ êµ¬ì„±í•˜ì„¸ìš”. ì†Œì œëª©ì€ `###` ë§ˆí¬ë‹¤ìš´ì„ ì‚¬ìš©í•˜ì„¸ìš”.
            -   ì†Œì œëª© ëª©ë¡:
{subtopic_lines}
            -   ê° ì†Œì œëª© ì•„ë˜ì—ëŠ” ìµœì†Œ 2-3ê°œì˜ ë¬¸ë‹¨ì„ ì‘ì„±í•˜ì—¬ ë‚´ìš©ì„ í’ë¶€í•˜ê²Œ ë§Œë“œì„¸ìš”.
            -   ë…ìì˜ ì´í•´ë¥¼ ë•ê¸° ìœ„í•´, ì „ë¬¸ ìš©ì–´ëŠ” ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•˜ê³ , í•„ìš”í•œ ê²½ìš° ì‹¤ì œ ì˜ˆì‹œë¥¼ ë“¤ì–´ì£¼ì„¸ìš”.
            -   ì œê³µëœ ì°¸ê³  ìë£Œ(ë‰´ìŠ¤, í•™ìˆ , ì¼ë°˜, í†µê³„)ë¥¼ ë³¸ë¬¸ ë‚´ìš©ì— ìì—°ìŠ¤ëŸ½ê²Œ ì¸ìš©í•˜ì—¬ ê¸€ì˜ ì‹ ë¢°ë„ë¥¼ ë†’ì—¬ì£¼ì„¸ìš”. í†µê³„ ìë£ŒëŠ” ìµœì†Œ 1ê°œ ì´ìƒ ë°˜ë“œì‹œ ì¸ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

        4.  **ì°¸ê³  ìë£Œ í™œìš©:**
            {research_text}
            {statistics_text}
            - **ì¤‘ìš” ì¸ìš© ì§€ì¹¨:** ë³¸ë¬¸ì—ì„œ [1], [2]ì™€ ê°™ì€ ì¸ìš©ë²ˆí˜¸ í‘œì‹œëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. ëŒ€ì‹  "X ë³´ê³ ì„œì— ë”°ë¥´ë©´" ë˜ëŠ” "Y ì—°êµ¬ ê²°ê³¼ì— ì˜í•˜ë©´" ë“± ì¶œì²˜ ì´ë¦„ì„ ì§ì ‘ ì–¸ê¸‰í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì¸ìš©í•˜ì„¸ìš”. ë³¸ë¬¸ì— URLì„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

        5.  **ìµœì í™” ìš”êµ¬ì‚¬í•­:**
            {optimization_requirements}

        6.  **[í•„ìˆ˜] ê²°ë¡  ì‘ì„± ê°€ì´ë“œ (ì‹ ë¢° êµ¬ì¶• ë° í–‰ë™ ìœ ë„):**
            -   ë³¸ë¬¸ì˜ í•µì‹¬ ë‚´ìš©ì„ ë‹¨ìˆœíˆ ìš”ì•½í•˜ëŠ” ê²ƒì„ ë„˜ì–´, ë…ìê°€ 'ì´ì œ ë¬´ì—‡ì„ í•´ì•¼ í• ì§€' ëª…í™•íˆ ì•Œ ìˆ˜ ìˆë„ë¡ í–‰ë™ ì§€ì¹¨ì„ ì œì‹œí•˜ë©° ë§ˆë¬´ë¦¬í•©ë‹ˆë‹¤.
            -   "ì˜¤ëŠ˜ ì•Œë ¤ë“œë¦° ë°©ë²•ì„ ë‹¹ì¥ ì ìš©í•´ë³´ì„¸ìš”." ì™€ ê°™ì´, ë…ìê°€ ì‹¤ì²œìœ¼ë¡œ ì˜®ê¸°ë„ë¡ ìì‹ ê°ì„ ë¶ˆì–´ë„£ê³  ê²©ë ¤í•´ì£¼ì„¸ìš”.
            -   **ê°€ì¥ ì¤‘ìš”:** "ë§Œì•½ ì•Œë ¤ë“œë¦° ë°©ë²•ìœ¼ë¡œë„ ë¬¸ì œê°€ í•´ê²°ë˜ì§€ ì•Šê±°ë‚˜, ìƒí™©ì´ ê¸‰ë°•í•˜ì—¬ ì „ë¬¸ê°€ì˜ ì¦‰ê°ì ì¸ ì¡°ì¹˜ê°€ í•„ìš”í•˜ë‹¤ë©´, í•œìˆœê°„ë„ ì£¼ì €í•˜ì§€ ë§ê³  ì €í¬ì—ê²Œ ì—°ë½ ì£¼ì„¸ìš”. ì‹ ì†í•˜ê²Œ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤." ë¼ëŠ” ë¬¸êµ¬ë¥¼ **ë°˜ë“œì‹œ í¬í•¨**í•˜ì—¬, ë…ìê°€ ë§‰ë§‰í•  ë•Œ ê¸°ëŒˆ ìˆ˜ ìˆëŠ” ë“ ë“ í•œ ì „ë¬¸ê°€ë¼ëŠ” ì¸ì‹ì„ ì‹¬ì–´ì£¼ì„¸ìš”.
            -   ë…ìì™€ì˜ ìƒí˜¸ì‘ìš©ì„ ìœ ë„í•˜ëŠ” ì§ˆë¬¸ì„ ë˜ì§€ì„¸ìš”. (ì˜ˆ: "'{keyword}'ì— ëŒ€í•´ ë” ê¶ê¸ˆí•œ ì ì´ ìˆë‹¤ë©´ ëŒ“ê¸€ë¡œ ì•Œë ¤ì£¼ì„¸ìš”.")

        7.  **ì°¸ê³  ìë£Œ ì„¹ì…˜:**
            -   ê¸€ì˜ ë§ˆì§€ë§‰ì—ëŠ” `## ì°¸ê³ ìë£Œ` ë¼ëŠ” ì œëª©ìœ¼ë¡œ ì„¹ì…˜ì„ ë§Œë“¤ê³ , ë³¸ë¬¸ ì‘ì„±ì— í™œìš©í•œ ëª¨ë“  ì°¸ê³  ìë£Œì˜ ì¶œì²˜ë¥¼ ëª…í™•í•˜ê²Œ ë°í˜€ì£¼ì„¸ìš”. (ì´ ì„¹ì…˜ì€ ê¸€ììˆ˜ ì¹´ìš´íŠ¸ì—ì„œ ì œì™¸ë©ë‹ˆë‹¤.)

        ---

        ì´ì œ ìœ„ì˜ ëª¨ë“  ì§€ì¹¨ì„ ì¢…í•©í•˜ì—¬, ë…ìì˜ ê¸°ëŒ€ë¥¼ ë›°ì–´ë„˜ëŠ” ê³ í’ˆì§ˆ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ ì‘ì„±ì„ ì‹œì‘í•´ ì£¼ì„¸ìš”.
        """
        return prompt
    
    def _create_verification_optimization_prompt(self, content, keyword, custom_morphemes, verification_result):
        morpheme_analysis_from_analyzer = verification_result.get('morpheme_analysis', {})
        
        base_morphemes = morpheme_analysis_from_analyzer['target_morphemes']['base']
        compound_morphemes = morpheme_analysis_from_analyzer['target_morphemes']['compound']
        current_counts = morpheme_analysis_from_analyzer['counts']

        target_min_base_morph = self.morpheme_analyzer.target_min_base_count
        target_max_base_morph = self.morpheme_analyzer.target_max_base_count
        target_min_compound_morph = self.morpheme_analyzer.target_min_compound_count
        target_max_compound_morph = self.morpheme_analyzer.target_max_compound_count

        target_min_chars = self.morpheme_analyzer.target_min_chars
        target_max_chars = self.morpheme_analyzer.target_max_chars
        
        morpheme_issues = []
        for morpheme in base_morphemes:
            info = current_counts.get(morpheme, {})
            count = info.get('count', 0)
            if not info.get('is_valid', True):
                morpheme_issues.append(f"- í•µì‹¬ ê¸°ë³¸ í˜•íƒœì†Œ '{morpheme}': í˜„ì¬ {count}íšŒ â†’ ëª©í‘œ {target_min_base_morph}-{target_max_base_morph}íšŒë¡œ ì¡°ì • í•„ìš”")
        
        for morpheme in compound_morphemes:
            info = current_counts.get(morpheme, {})
            count = info.get('count', 0)
            if not info.get('is_valid', True):
                morpheme_issues.append(f"- ë³µí•© í‚¤ì›Œë“œ/êµ¬ë¬¸ '{morpheme}': í˜„ì¬ {count}íšŒ â†’ ëª©í‘œ {target_min_compound_morph}-{target_max_compound_morph}íšŒë¡œ ì¡°ì • í•„ìš”")
        
        morpheme_issues_text = "\n".join(morpheme_issues) if morpheme_issues else "ëª¨ë“  ëª©í‘œ í˜•íƒœì†Œê°€ ì ì • ë²”ìœ„ ë‚´ì— ìˆìŠµë‹ˆë‹¤."
        
        char_count = verification_result['char_count']
        char_count_guidance = ""
        if char_count < target_min_chars:
            char_count_guidance = f"ê¸€ììˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. í˜„ì¬ {char_count}ì. {target_min_chars-char_count}ì ì´ìƒ ëŠ˜ë ¤ {target_min_chars}-{target_max_chars}ìë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”."
        elif char_count > target_max_chars:
            char_count_guidance = f"ê¸€ììˆ˜ê°€ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. í˜„ì¬ {char_count}ì. {char_count-target_max_chars}ì ì´ìƒ ì¤„ì—¬ {target_min_chars}-{target_max_chars}ìë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”."
        else:
            char_count_guidance = f"ê¸€ììˆ˜ëŠ” ì ì • ë²”ìœ„({target_min_chars}-{target_max_chars}ì)ì…ë‹ˆë‹¤. í˜•íƒœì†Œ ì¡°ì • ì‹œ ì´ ë²”ìœ„ë¥¼ ìœ ì§€í•´ì£¼ì„¸ìš”."
        
        optimization_strategies = self._generate_dynamic_optimization_strategies(keyword, current_counts, base_morphemes + compound_morphemes)
        
        return f"""
        ë‹¤ìŒ ë¸”ë¡œê·¸ ì½˜í…ì¸ ë¥¼ ìµœì í™”í•´ì£¼ì„¸ìš”. ë‹¤ìŒ ì¡°ê±´ì„ ëª¨ë‘ ì¶©ì¡±í•˜ë„ë¡ ìˆ˜ì •í•´ì£¼ì„¸ìš”:
        
        ========== ìµœì í™” ëª©í‘œ ==========
        
        1. ê¸€ììˆ˜ ì¡°ê±´: {target_min_chars}-{target_max_chars}ì (ê³µë°± ì œì™¸)
           {char_count_guidance}
        
        2. ëª©í‘œ í˜•íƒœì†Œ ì¶œí˜„ íšŸìˆ˜ ì¡°ê±´:
           {morpheme_issues_text}

        3. í‚¤ì›Œë“œ ë° í˜•íƒœì†Œ ì¹´ìš´íŒ… ë°©ì‹:
           - í•µì‹¬ ê¸°ë³¸ í˜•íƒœì†Œ (ì˜ˆ: 'ì—”ì§„', 'ì˜¤ì¼', 'ì¢…ë¥˜'): ë¬¸ì¥ ë‚´ì—ì„œ ë¶€ë¶„ì ìœ¼ë¡œ í¬í•¨ë˜ì–´ë„ ì¹´ìš´íŠ¸ë©ë‹ˆë‹¤. (ì˜ˆ: 'ì—”ì§„ì˜¤ì¼ì¢…ë¥˜'ì—ì„œ 'ì—”ì§„' 1íšŒ, 'ì˜¤ì¼' 1íšŒ, 'ì¢…ë¥˜' 1íšŒ)
           - ë³µí•© í‚¤ì›Œë“œ ë° êµ¬ë¬¸ (ì˜ˆ: 'ì—”ì§„ì˜¤ì¼', 'ì—”ì§„ì˜¤ì¼ì¢…ë¥˜'): ì •í™•íˆ í•´ë‹¹ êµ¬ë¬¸ì´ ì¼ì¹˜í•´ì•¼ ì¹´ìš´íŠ¸ë©ë‹ˆë‹¤.

        ========== ìµœì í™” ì „ëµ ==========
        {optimization_strategies}
        
        ========== ì¤‘ìš” ì§€ì¹¨ ==========
        
        1. ì½˜í…ì¸ ì˜ í•µì‹¬ ë©”ì‹œì§€ì™€ ì „ë¬¸ì„±ì€ ìœ ì§€í•˜ì„¸ìš”.
        2. ëª¨ë“  ì†Œì œëª©ê³¼ ì£¼ìš” ì„¹ì…˜ì„ ìœ ì§€í•˜ì„¸ìš”.
        3. ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì²´ì™€ íë¦„ì„ ìœ ì§€í•˜ì„¸ìš”.
        4. ëª¨ë“  í†µê³„ ìë£Œ ì¸ìš©ê³¼ ì¶œì²˜ í‘œì‹œë¥¼ ìœ ì§€í•˜ì„¸ìš”.
        5. ì¡°ì • í›„ì—ëŠ” ë°˜ë“œì‹œ ìœ„ì— ì–¸ê¸‰ëœ ê° ëª©í‘œ í˜•íƒœì†Œê°€ ì§€ì •ëœ ë²”ìœ„ ë‚´ì—ì„œ ì‚¬ìš©ë˜ì—ˆëŠ”ì§€, ê¸€ììˆ˜ê°€ ë§ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
        6. ê²°ê³¼ë¬¼ë§Œ ì œì‹œí•˜ê³  ì¶”ê°€ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.
        
        ========== ì›ë³¸ ì½˜í…ì¸  ==========
        {content}
        """
    
    def _generate_dynamic_optimization_strategies(self, keyword, current_morpheme_counts, all_target_morphemes_list):
        excess_morphemes = []
        lacking_morphemes = []
        
        # Use MorphemeAnalyzer to get target ranges for each type
        ma = self.morpheme_analyzer # shorthand
        
        for morpheme in all_target_morphemes_list:
            info = current_morpheme_counts.get(morpheme, {})
            count = info.get('count', 0)
            morpheme_type = info.get('type')

            if morpheme_type == 'base':
                target_min = ma.target_min_base_count
                target_max = ma.target_max_base_count
            elif morpheme_type == 'compound':
                target_min = ma.target_min_compound_count
                target_max = ma.target_max_compound_count
            else: # Should not happen with proper categorization
                continue

            if count > target_max:
                excess_morphemes.append(morpheme)
            elif count < target_min:
                lacking_morphemes.append(morpheme)
        
        strategies = """
        1. ê³¼ë‹¤ ì‚¬ìš©ëœ ëª©í‘œ í˜•íƒœì†Œ ê°ì†Œ ë°©ë²•:
           - ë™ì˜ì–´/ìœ ì‚¬ì–´ ëŒ€ì²´: (ë‹¨, ëŒ€ì²´ì–´ëŠ” ëª©í‘œ í˜•íƒœì†Œê°€ ì•„ë‹ˆì–´ì•¼ í•˜ë©°, í•´ë‹¹ í˜•íƒœì†Œì˜ ì¹´ìš´íŒ… ë°©ì‹(ë¶€ë¶„/ì •í™•)ì„ ê³ ë ¤í•˜ì—¬ ëŒ€ì²´)
           - ì§€ì‹œì–´ ì‚¬ìš©: "ì´ê²ƒ", "ê·¸ê²ƒ", "í•´ë‹¹ ë‚´ìš©" ë“±ìœ¼ë¡œ ëŒ€ì²´
           - ìì—°ìŠ¤ëŸ¬ìš´ ìƒëµ: ë¬¸ë§¥ìƒ ì´í•´ ê°€ëŠ¥í•œ ê²½ìš° ìƒëµ
           - ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ë¬¸ì¥ ì¬êµ¬ì„±: ê°™ì€ ì˜ë¯¸ë¥¼ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ í‘œí˜„
           - í•´ë‹¹ í˜•íƒœì†Œê°€ í¬í•¨ëœ ë¬¸ì¥ ì „ì²´ë¥¼ ë¬¸ë§¥ìƒ ìì—°ìŠ¤ëŸ½ê²Œ ì‚­ì œí•˜ê±°ë‚˜, í˜•íƒœì†Œë§Œ ì œê±°í•˜ì—¬ ë¬¸ì¥ì„ ê°„ê²°í•˜ê²Œ ë§Œë“œì„¸ìš”.

        2. ë¶€ì¡±í•œ ëª©í‘œ í˜•íƒœì†Œ ì¦ê°€ ë°©ë²•:
           - êµ¬ì²´ì ì¸ ì˜ˆì‹œë‚˜ ì‚¬ë¡€ ì¶”ê°€: í•´ë‹¹ ëª©í‘œ í˜•íƒœì†Œê°€ í¬í•¨ëœ ì˜ˆì‹œ ì¶”ê°€
           - ì„¤ëª… í™•ì¥: í•µì‹¬ ê°œë…ì— ëŒ€í•œ ì¶”ê°€ ì„¤ëª… ì œê³µ (ëª©í‘œ í˜•íƒœì†Œ ì‚¬ìš©)
           - ì‹¤ìš©ì ì¸ íŒì´ë‚˜ ì¡°ì–¸ ì¶”ê°€: ëª©í‘œ í˜•íƒœì†Œê°€ í¬í•¨ëœ íŒ ì œì‹œ
           - ê¸°ì¡´ ë¬¸ì¥ ë¶„ë¦¬ ë˜ëŠ” í™•ì¥: í•œ ë¬¸ì¥ì„ ë‘ ê°œë¡œ ë‚˜ëˆ„ê±°ë‚˜ í™•ì¥í•˜ì—¬ ëª©í‘œ í˜•íƒœì†Œ ì‚¬ìš© ê¸°íšŒ ì¦ê°€
        """
        
        substitution_text = "\n3. ìœ ìš©í•œ ëŒ€ì²´ì–´ ì˜ˆì‹œ (ê³¼ë‹¤ í˜•íƒœì†Œ ê°ì†Œ ì‹œ):"
        added_subs = False
        
        # Suggest substitutions for excess morphemes
        for morpheme in excess_morphemes:
            morpheme_substitutions = self.substitution_generator.get_substitutions(keyword, morpheme)
            if morpheme_substitutions:
                substitution_text += f"\n   - '{morpheme}' ëŒ€ì²´ì–´: {', '.join(morpheme_substitutions[:3])}"
                added_subs = True
        
        return strategies + (substitution_text if added_subs else "")
        
    def _add_references(self, content, research_data):
        if "## ì°¸ê³ ìë£Œ" in content: return content
        
        references_to_add = []
        for source_type_key in ['news', 'academic', 'general']:
            for source_item in research_data.get(source_type_key, []):
                if self._find_citation_in_content(content, source_item):
                    if not any(ref['url'] == source_item.get('url') for ref in references_to_add if source_item.get('url')):
                        references_to_add.append({
                            'title': source_item.get('title', 'ì œëª© ì—†ìŒ'),
                            'url': source_item.get('url', '#'),
                            'source': source_item.get('source', '')
                        })
        
        for stat_item in research_data.get('statistics', []):
            source_url = stat_item.get('source_url', '')
            source_title = stat_item.get('source_title', '')
            if source_url and source_title and (source_title.lower() in content.lower() or (stat_item.get('source','').lower() in content.lower() and stat_item.get('source',''))):
                if not any(ref['url'] == source_url for ref in references_to_add):
                     references_to_add.append({
                        'title': source_title,
                        'url': source_url,
                        'source': stat_item.get('source', '')
                    })

        if not references_to_add: return content
        
        reference_section_text = "\n\n## ì°¸ê³ ìë£Œ\n"
        for i, ref_item in enumerate(references_to_add, 1):
            ref_source_text = f" - {ref_item['source']}" if ref_item['source'] else ""
            reference_section_text += f"{i}. [{ref_item['title']}]({ref_item['url']}){ref_source_text}\n"
        
        return content.strip() + reference_section_text

    def _extract_references(self, content_with_refs):
        extracted_refs = []
        if "## ì°¸ê³ ìë£Œ" in content_with_refs:
            refs_section_text = content_with_refs.split("## ì°¸ê³ ìë£Œ", 1)[1]
            link_pattern = re.compile(r'\[(.*?)\]\((.*?)\)(?: - (.*?))?\n')
            matches = link_pattern.findall(refs_section_text)
            
            for title, url, source in matches:
                extracted_refs.append({
                    'title': title.strip(), 'url': url.strip(),
                    'source': source.strip() if source else ''
                })
        return extracted_refs

    def _format_for_mobile(self, content):
        lines = content.split('\n')
        formatted_lines = []
        in_code_block = False

        for line in lines:
            stripped_line = line.strip()
            if stripped_line.startswith('```'):
                in_code_block = not in_code_block
                formatted_lines.append(line)
                continue
            
            if in_code_block or \
               stripped_line.startswith(('#', '##', '###')) or \
               not stripped_line or \
               stripped_line.startswith(('- ', '* ', '+ ')) or \
               re.match(r'^\d+\.\s', stripped_line) or \
               stripped_line.startswith('>'):
                formatted_lines.append(line)
                continue
            
            words = line.split()
            current_formatted_line = ""
            for word in words:
                temp_line_char_len = len((current_formatted_line + " " + word).replace(" ", ""))
                if temp_line_char_len > 23 and current_formatted_line:
                    formatted_lines.append(current_formatted_line)
                    current_formatted_line = word
                else:
                    current_formatted_line = (current_formatted_line + " " + word).strip()
            
            if current_formatted_line:
                formatted_lines.append(current_formatted_line)
        
        return '\n'.join(formatted_lines)
    
    def _find_citation_in_content(self, content_text, source_info_dict):
        content_lower = content_text.lower()
        
        source_name_candidates = [
            source_info_dict.get('source', '').lower(),
            source_info_dict.get('author', '').lower()
        ]
        source_name_candidates = [name for name in source_name_candidates if name and len(name) > 2]

        for name_candidate in source_name_candidates:
            if name_candidate in content_lower:
                return True
        
        title_lower = source_info_dict.get('title', '').lower()
        if title_lower:
            title_words = title_lower.split()
            for i in range(len(title_words) - 1):
                phrase = " ".join(title_words[i:i+2])
                if len(phrase) > 5 and phrase in content_lower: return True
            if len(title_words) >=3:
                phrase = " ".join(title_words[:3])
                if len(phrase) > 8 and phrase in content_lower: return True

        snippet_lower = source_info_dict.get('snippet', '').lower()
        if snippet_lower:
            numbers_in_snippet = re.findall(r'\d+(?:[.,]\d+)?%?', snippet_lower)
            key_phrases_in_snippet = re.findall(r'\b\w+\s\w+\s\w+\b', snippet_lower)

            citation_keywords = ["ë”°ë¥´ë©´", "ì—°êµ¬", "ì¡°ì‚¬", "ë³´ê³ ì„œ", "ë°œí‘œ", "í†µê³„", "ìë£Œ", "ì œì‹œ"]
            for num_in_snip in numbers_in_snippet:
                if num_in_snip in content_lower:
                    idx = content_lower.find(num_in_snip)
                    context_around_num = content_lower[max(0, idx-30):min(len(content_lower), idx+len(num_in_snip)+30)]
                    if any(cit_kw in context_around_num for cit_kw in citation_keywords):
                        return True
            
            for phrase_in_snip in key_phrases_in_snippet:
                if len(phrase_in_snip) > 8 and phrase_in_snip in content_lower:
                     idx = content_lower.find(phrase_in_snip)
                     context_around_phrase = content_lower[max(0, idx-30):min(len(content_lower), idx+len(phrase_in_snip)+30)]
                     if any(cit_kw in context_around_phrase for cit_kw in citation_keywords):
                        return True
        return False
