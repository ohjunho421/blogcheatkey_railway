# d:\BlogCheatKey\blog_cheatkey_v2\blog_cheatkey\backend\content\services\optimizer.py
import re
import json
import logging
import time
import random
import traceback
from django.conf import settings
from konlpy.tag import Okt 
import google.generativeai as genai
from content.models import BlogContent, MorphemeAnalysis
from .formatter import ContentFormatter
from .substitution_generator import SubstitutionGenerator
from .morpheme_analyzer import MorphemeAnalyzer 

logger = logging.getLogger(__name__)

class ContentOptimizer:
    """
    Gemini APIë¥¼ ì‚¬ìš©í•œ ë¸”ë¡œê·¸ ì½˜í…ì¸  ìµœì í™” í´ë˜ìŠ¤
    ì£¼ìš” ê¸°ëŠ¥: ê¸€ììˆ˜, í‚¤ì›Œë“œ ì¶œí˜„ íšŸìˆ˜ í™•ì¸ ë° ìµœì í™”
    """

    def __init__(self):
        self.google_api_key = settings.GOOGLE_API_KEY
        genai.configure(api_key=self.google_api_key)
        self.model = genai.GenerativeModel('gemini-2.5-pro')
        self.okt = Okt() 
        self.substitution_generator = SubstitutionGenerator()
        self.morpheme_analyzer = MorphemeAnalyzer()

    def optimize_existing_content_v3(self, content_id):
        """
        ê¸°ì¡´ ì½˜í…ì¸ ë¥¼ SEO ì¹œí™”ì ìœ¼ë¡œ ìµœì í™”

        Args:
            content_id (int): BlogContent ëª¨ë¸ì˜ ID

        Returns:
            dict: ìµœì í™” ê²°ê³¼
        """
        try:
            blog_content = BlogContent.objects.get(id=content_id)
            original_content_text = blog_content.content # API í˜¸ì¶œ ì „ ì›ë³¸ ì €ì¥
            keyword = blog_content.keyword.keyword
            custom_morphemes_for_analysis = None # Assuming this is passed from higher level

            logger.info(f"ì½˜í…ì¸  SEO ìµœì í™” ì‹œì‘ (V3): content_id={content_id}, í‚¤ì›Œë“œ={keyword}")

            api_optimized_content = None
            best_api_analysis = self.morpheme_analyzer.analyze(original_content_text, keyword, custom_morphemes_for_analysis) # ì´ˆê¸° ë¶„ì„ì€ ì›ë³¸ ê¸°ì¤€

            api_attempts_count = 0

            for attempt in range(3): # Still keep a few API attempts for initial optimization
                api_attempts_count = attempt + 1
                try:
                    content_for_api_prompt = api_optimized_content if api_optimized_content else original_content_text
                    current_analysis_for_prompt = self.morpheme_analyzer.analyze(content_for_api_prompt, keyword, custom_morphemes_for_analysis)

                    if attempt == 0:
                        prompt = self._create_seo_optimization_prompt(content_for_api_prompt, keyword, custom_morphemes_for_analysis, current_analysis_for_prompt)
                        temp = 0.7
                    elif attempt == 1:
                        prompt = self._create_seo_readability_prompt(content_for_api_prompt, keyword, custom_morphemes_for_analysis, current_analysis_for_prompt)
                        temp = 0.5
                    else:
                        prompt = self._create_ultra_seo_prompt(content_for_api_prompt, keyword, custom_morphemes_for_analysis, current_analysis_for_prompt)
                        temp = 0.3
                    
                    logger.info(f"API ìµœì í™” ì‹œë„ #{attempt+1}/3, temperature={temp}")

                    response = self.model.generate_content(
                        prompt,
                        generation_config=genai.types.GenerationConfig(
                            temperature=temp,
                            max_output_tokens=4096
                        )
                    )
                    
                    current_api_output = response.text
                    analysis_of_api_output = self.morpheme_analyzer.analyze(current_api_output, keyword, custom_morphemes_for_analysis)
                    
                    logger.info(f"API ì‹œë„ #{attempt+1} ê²°ê³¼: ê¸€ììˆ˜={analysis_of_api_output['char_count']}, ëª©í‘œí˜•íƒœì†Œ ìœ íš¨={analysis_of_api_output['is_valid_morphemes']}")
                    
                    if self.morpheme_analyzer.is_better_optimization(analysis_of_api_output, best_api_analysis):
                        api_optimized_content = current_api_output
                        best_api_analysis = analysis_of_api_output
                        logger.info(f"ìƒˆë¡œìš´ ìµœìƒì˜ API ê²°ê³¼ ë°œê²¬: ê¸€ììˆ˜={best_api_analysis['char_count']}, ëª©í‘œí˜•íƒœì†Œ ìœ íš¨={best_api_analysis['is_valid_morphemes']}")
                    
                    if best_api_analysis['is_fully_optimized']:
                        logger.info("API ìµœì í™” ì„±ê³µ: ëª¨ë“  ì¡°ê±´ ì¶©ì¡±")
                        break
                        
                except Exception as e:
                    logger.error(f"API ìµœì í™” ì‹œë„ #{attempt+1} ì˜¤ë¥˜: {str(e)}")
                    logger.error(traceback.format_exc())
                    time.sleep(5)

            content_to_force_optimize = api_optimized_content if api_optimized_content else original_content_text
            
            logger.info("SEO ê°•ì œ ìµœì í™” ì‹œì‘")
            final_optimized_content = self.enforce_seo_optimization(content_to_force_optimize, keyword, custom_morphemes_for_analysis)
            
            final_analysis = self.morpheme_analyzer.analyze(final_optimized_content, keyword, custom_morphemes_for_analysis)
            logger.info(f"ìµœì¢… ê²°ê³¼: ê¸€ììˆ˜={final_analysis['char_count']}, ëª©í‘œí˜•íƒœì†Œ ìœ íš¨={final_analysis['is_valid_morphemes']}")
            
            formatter = ContentFormatter()
            mobile_formatted_content = formatter.format_for_mobile(final_optimized_content)
            
            blog_content.content = final_optimized_content
            blog_content.mobile_formatted_content = mobile_formatted_content
            blog_content.char_count = final_analysis['char_count']
            blog_content.is_optimized = final_analysis['is_fully_optimized']
            
            meta_data = {
                'original_char_count': len(original_content_text.replace(" ", "")),
                'final_char_count': final_analysis['char_count'],
                'is_valid_char_count': final_analysis['is_valid_char_count'],
                'is_valid_morphemes': final_analysis['is_valid_morphemes'],
                'optimization_date': time.strftime("%Y-%m-%d %H:%M:%S"),
                'algorithm_version': 'v3_analyzer_focused_v3', # Updated version
                'api_attempts': api_attempts_count 
            }
            blog_content.meta_data = meta_data
            blog_content.save()
            
            blog_content.morpheme_analyses.all().delete()
            
            if 'morpheme_analysis' in final_analysis and 'counts' in final_analysis['morpheme_analysis']:
                for morpheme, info in final_analysis['morpheme_analysis']['counts'].items():
                    MorphemeAnalysis.objects.create(
                        content=blog_content,
                        morpheme=morpheme,
                        count=info.get('count', 0),
                        is_valid=info.get('is_valid', False),
                        morpheme_type=info.get('type', 'unknown')
                    )
            
            success_message = "ì½˜í…ì¸ ê°€ ì„±ê³µì ìœ¼ë¡œ SEO ìµœì í™”ë˜ì—ˆìŠµë‹ˆë‹¤."
            if not final_analysis['is_fully_optimized']:
                success_message += " (ì¼ë¶€ ì¡°ê±´ ë¯¸ë‹¬ì„±)"
            
            logger.info(f"ì½˜í…ì¸  SEO ìµœì í™” ì™„ë£Œ: ID={content_id}, ê¸€ììˆ˜={final_analysis['char_count']}, ëª¨ë“  ëª©í‘œí˜•íƒœì†Œ ìœ íš¨={final_analysis['is_valid_morphemes']}")
                
            return {
                'success': True,
                'message': success_message,
                'content_id': content_id,
                'is_valid_char_count': final_analysis['is_valid_char_count'],
                'is_valid_morphemes': final_analysis['is_valid_morphemes'],
                'char_count': final_analysis['char_count'],
                'attempts': api_attempts_count,
                'algorithm_version': 'v3_analyzer_focused_v3'
            }
                
        except BlogContent.DoesNotExist:
            logger.error(f"ID {content_id}ì— í•´ë‹¹í•˜ëŠ” ì½˜í…ì¸ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {
                'success': False,
                'message': f"ID {content_id}ì— í•´ë‹¹í•˜ëŠ” ì½˜í…ì¸ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                'content_id': content_id
            }
        except Exception as e:
            logger.error(f"ì½˜í…ì¸  ìµœì í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            logger.error(traceback.format_exc())
            return {
                'success': False,
                'message': f"ì½˜í…ì¸  ìµœì í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                'content_id': content_id
            }

    def enforce_seo_optimization(self, content, keyword, custom_morphemes=None):
        """
        SEO ìµœì í™”ë¥¼ ìœ„í•œ ê°•ì œ ë³€í™˜ (MorphemeAnalyzer ì‚¬ìš©)

        Args:
            content (str): ìµœì í™”í•  ì½˜í…ì¸ 
            keyword (str): ì£¼ìš” í‚¤ì›Œë“œ
            custom_morphemes (list): ì‚¬ìš©ì ì§€ì • í˜•íƒœì†Œ

        Returns:
            str: SEO ìµœì í™”ëœ ì½˜í…ì¸ 
        """
        content_parts = self.separate_content_and_refs(content)
        content_without_refs = content_parts['content_without_refs']
        refs_section = content_parts['refs_section']

        initial_analysis = self.morpheme_analyzer.analyze(content_without_refs, keyword, custom_morphemes)
        logger.info(f"SEO ê°•ì œ ìµœì í™” ì‹œì‘: ê¸€ììˆ˜={initial_analysis['char_count']} (ìœ íš¨: {initial_analysis['is_valid_char_count']}), ëª©í‘œí˜•íƒœì†Œ ìœ íš¨={initial_analysis['is_valid_morphemes']}")

        if initial_analysis['is_fully_optimized']:
            logger.info("ì´ë¯¸ SEO ìµœì í™”ëœ ìƒíƒœì…ë‹ˆë‹¤.")
            if refs_section and "## ì°¸ê³ ìë£Œ" not in content_without_refs:
                 return content_without_refs + "\n\n" + refs_section
            return content_without_refs

        optimized_content = content_without_refs
        optimized_content = self._improve_content_structure(optimized_content, keyword)
        optimized_content = self._optimize_headings(optimized_content, keyword)

        attempt = 0
        previous_content = ""
        max_safety_attempts = 100 # Safety break for infinite loop

        while attempt < max_safety_attempts:
            if optimized_content == previous_content:
                logger.warning("ìµœì í™” ê³¼ì •ì´ ê³ ì°© ìƒíƒœì— ë¹ ì¡ŒìŠµë‹ˆë‹¤. ë£¨í”„ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break
            previous_content = optimized_content

            current_analysis = self.morpheme_analyzer.analyze(optimized_content, keyword, custom_morphemes)
            logger.info(f"ê°•ì œ ìµœì í™” ì‹œë„ #{attempt+1}: ê¸€ììˆ˜={current_analysis['char_count']} (ìœ íš¨: {current_analysis['is_valid_char_count']}), ëª©í‘œí˜•íƒœì†Œ ìœ íš¨={current_analysis['is_valid_morphemes']}")

            if current_analysis['is_fully_optimized']:
                logger.info("ê°•ì œ ìµœì í™” ì„±ê³µ: ëª¨ë“  ì¡°ê±´ ì¶©ì¡±")
                break

            needs_char_adjustment = not current_analysis['is_valid_char_count']
            needs_morpheme_adjustment = not current_analysis['is_valid_morphemes'] 

            # í˜•íƒœì†Œ ì¡°ì •ì´ ìš°ì„ ìˆœìœ„ê°€ ë†’ìŒ
            if needs_morpheme_adjustment:
                logger.info("ì¡°ì •: ëª©í‘œ í˜•íƒœì†Œ")
                optimized_content = self._enforce_exact_target_morpheme_count(
                    optimized_content, 
                    keyword, 
                    custom_morphemes,
                    current_analysis['morpheme_analysis']['counts'],
                    current_analysis['morpheme_analysis']['target_morphemes']
                )
            elif needs_char_adjustment:
                logger.info("ì¡°ì •: ê¸€ììˆ˜")
                target_chars_center = (self.morpheme_analyzer.target_min_chars + self.morpheme_analyzer.target_max_chars) // 2
                optimized_content = self._enforce_exact_char_count_v2(
                    optimized_content, 
                    target_chars_center, 
                    tolerance=100 + attempt * 10, # í—ˆìš© ì˜¤ì°¨ë¥¼ ì ì§„ì ìœ¼ë¡œ ëŠ˜ë¦¼
                    all_target_morphemes=current_analysis['morpheme_analysis']['target_morphemes'], # Pass categorized morphemes
                    current_morpheme_counts=current_analysis['morpheme_analysis']['counts']
                )
            
            attempt += 1
        
        # ğŸ‘‡ [ê°œì„ ] ìµœì¢…ì ìœ¼ë¡œ 20íšŒë¥¼ ì´ˆê³¼í•˜ëŠ” í˜•íƒœì†Œê°€ ì—†ë„ë¡ ê°•ì œ ì¡°ì •
        logger.info("ìµœì¢… ê²€ì¦: 20íšŒ ì´ˆê³¼ í˜•íƒœì†Œ ê°•ì œ ì¡°ì • ì‹œì‘")
        optimized_content = self._enforce_absolute_max_count(optimized_content, keyword, custom_morphemes, max_count=20)
            
        optimized_content = self._optimize_paragraph_breaks(optimized_content)

        if refs_section and "## ì°¸ê³ ìë£Œ" not in optimized_content:
            optimized_content = optimized_content + "\n\n" + refs_section
        return optimized_content

    def _enforce_absolute_max_count(self, content, keyword, custom_morphemes, max_count):
        """
        ëª¨ë“  ëª©í‘œ í˜•íƒœì†Œê°€ ì§€ì •ëœ ìµœëŒ€ íšŸìˆ˜(max_count)ë¥¼ ë„˜ì§€ ì•Šë„ë¡ ê°•ì œë¡œ ì¡°ì •í•©ë‹ˆë‹¤.
        """
        safety_break = 0
        while safety_break < 20: # ë¬´í•œ ë£¨í”„ ë°©ì§€
            analysis = self.morpheme_analyzer.analyze(content, keyword, custom_morphemes)
            morphemes_over_limit = []

            for morpheme, info in analysis['morpheme_analysis']['counts'].items():
                if info['count'] > max_count:
                    morphemes_over_limit.append((morpheme, info['count']))
            
            if not morphemes_over_limit:
                logger.info(f"ìµœì¢… ê²€ì¦ ì™„ë£Œ: ëª¨ë“  ëª©í‘œ í˜•íƒœì†Œê°€ {max_count}íšŒ ì´í•˜ì…ë‹ˆë‹¤.")
                return content
            
            # ê°€ì¥ ë§ì´ ì´ˆê³¼ëœ í˜•íƒœì†Œë¶€í„° ì²˜ë¦¬
            morphemes_over_limit.sort(key=lambda x: x[1], reverse=True)
            morpheme_to_reduce, current_count = morphemes_over_limit[0]
            
            logger.warning(f"ìµœì¢… ê²€ì¦: í˜•íƒœì†Œ '{morpheme_to_reduce}'ê°€ {max_count}íšŒë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤ ({current_count}íšŒ). 19íšŒë¡œ ê°•ì œ ì¡°ì •í•©ë‹ˆë‹¤.")
            
            content = self._reduce_morpheme_to_target(
                content,
                morpheme_to_reduce,
                target_count=max_count - 1, # ëª©í‘œ íšŸìˆ˜ë¥¼ 19ë¡œ ì„¤ì •í•˜ì—¬ í™•ì‹¤íˆ ì¤„ì„
                all_target_morphemes_dict=analysis['morpheme_analysis']['target_morphemes']
            )
            safety_break += 1
        
        logger.error(f"ìµœì¢… ê²€ì¦ ì‹¤íŒ¨: {safety_break}íšŒ ì‹œë„ í›„ì—ë„ 20íšŒë¥¼ ì´ˆê³¼í•˜ëŠ” í˜•íƒœì†Œê°€ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤.")
        return content

    def _improve_content_structure(self, content, keyword):
        logger.debug(f"ì½˜í…ì¸  êµ¬ì¡° ê°œì„  ì‹œë„: {keyword}")
        return content

    def _optimize_headings(self, content, keyword):
        logger.debug(f"ì œëª© ìµœì í™” ì‹œë„: {keyword}")
        return content

    def _optimize_paragraph_breaks(self, content):
        logger.debug("ë¬¸ë‹¨ ê°„ê²© ë° ì¤„ë°”ê¿ˆ ìµœì í™” ì‹œë„")
        return content

    def _force_adjust_target_morphemes_extreme(self, content, keyword, custom_morphemes, current_morpheme_counts, target_morphemes_dict):
        """
        'ëª©í‘œ' í˜•íƒœì†Œ ì¶œí˜„ íšŸìˆ˜ë¥¼ ë§ˆì§€ë§‰ ìˆ˜ë‹¨ìœ¼ë¡œ ê°•ì œ ì¡°ì • (MorphemeAnalyzer ì‚¬ìš©)
        target_morphemes_dict now contains 'base' and 'compound' lists.
        """
        logger.warning("ê·¹ë‹¨ì  ëª©í‘œ í˜•íƒœì†Œ ì¡°ì • ì‹œì‘")
        adjusted_content = content
        
        base_morphemes = target_morphemes_dict['base']
        compound_morphemes = target_morphemes_dict['compound']
        all_target_morphemes_list = target_morphemes_dict['all_list']

        # Adjust base morphemes first
        for morpheme in base_morphemes:
            current_info = current_morpheme_counts.get(morpheme, {'count': 0, 'is_valid': False})
            current_count = current_info['count']
            
            target_min = self.morpheme_analyzer.target_min_base_count
            target_max = self.morpheme_analyzer.target_max_base_count
            target_count = random.randint(target_min, target_max) # Aim for a random valid count

            if current_count > target_max:
                logger.warning(f"í•µì‹¬ ê¸°ë³¸ í˜•íƒœì†Œ '{morpheme}' ê³¼ë‹¤: {current_count}íšŒ -> ëª©í‘œ {target_count}íšŒë¡œ ì¤„ì„")
                adjusted_content = self._reduce_morpheme_to_target(
                    adjusted_content, 
                    morpheme, 
                    target_count, 
                    target_morphemes_dict # Pass the full dict
                )
            elif current_count < target_min:
                shortage = target_min - current_count
                logger.warning(f"í•µì‹¬ ê¸°ë³¸ í˜•íƒœì†Œ '{morpheme}' ë¶€ì¡±: {current_count}íšŒ -> {target_min}íšŒë¡œ ëŠ˜ë¦¼ (ì¶”ê°€ëŸ‰: {shortage}íšŒ)")
                adjusted_content = self._add_morpheme_strategically(adjusted_content, morpheme, shortage)

        # Adjust compound morphemes next
        for morpheme in compound_morphemes:
            current_info = current_morpheme_counts.get(morpheme, {'count': 0, 'is_valid': False})
            current_count = current_info['count']
            
            target_min = self.morpheme_analyzer.target_min_compound_count
            target_max = self.morpheme_analyzer.target_max_compound_count
            target_count = random.randint(target_min, target_max) # Aim for a random valid count

            if current_count > target_max:
                logger.warning(f"ë³µí•© í‚¤ì›Œë“œ/êµ¬ë¬¸ '{morpheme}' ê³¼ë‹¤: {current_count}íšŒ -> ëª©í‘œ {target_count}íšŒë¡œ ì¤„ì„")
                adjusted_content = self._reduce_morpheme_to_target(
                    adjusted_content, 
                    morpheme, 
                    target_count, 
                    target_morphemes_dict # Pass the full dict
                )
            elif current_count < target_min:
                shortage = target_min - current_count
                logger.warning(f"ë³µí•© í‚¤ì›Œë“œ/êµ¬ë¬¸ '{morpheme}' ë¶€ì¡±: {current_count}íšŒ -> {target_min}íšŒë¡œ ëŠ˜ë¦¼ (ì¶”ê°€ëŸ‰: {shortage}íšŒ)")
                adjusted_content = self._add_morpheme_strategically(adjusted_content, morpheme, shortage)
        
        # Final verification log
        final_analysis_after_extreme = self.morpheme_analyzer.analyze(adjusted_content, keyword, custom_morphemes)
        logger.warning("ê·¹ë‹¨ì  ëª©í‘œ í˜•íƒœì†Œ ì¡°ì • í›„ ìµœì¢… ê²°ê³¼:")
        for morpheme_type_key, morphemes_list in target_morphemes_dict.items():
            if morpheme_type_key == 'all_list': continue
            for morpheme in morphemes_list:
                info = final_analysis_after_extreme['morpheme_analysis']['counts'].get(morpheme, {})
                count = info.get('count',0)
                if info.get('type') == 'base':
                    target_min = self.morpheme_analyzer.target_min_base_count
                    target_max = self.morpheme_analyzer.target_max_base_count
                else: # compound
                    target_min = self.morpheme_analyzer.target_min_compound_count
                    target_max = self.morpheme_analyzer.target_max_compound_count
                status = "ì ì •" if target_min <= count <= target_max else "ë¶€ì ì •"
                logger.warning(f"- '{morpheme}' ({info.get('type')}): {count}íšŒ ({status})")
        return adjusted_content
    
    def _add_morpheme_strategically(self, content, morpheme, count_to_add):
        logger.info(f"í˜•íƒœì†Œ '{morpheme}' {count_to_add}íšŒ ì „ëµì ìœ¼ë¡œ ì¶”ê°€")
        paragraphs = content.split("\n\n")
        normal_paragraphs_indices = [i for i, p in enumerate(paragraphs)
                                     if not p.strip().startswith(('#', '##', '###')) and len(p.strip()) > 50]

        if not normal_paragraphs_indices:
            logger.warning(f"'{morpheme}' ì¶”ê°€í•  ì ì ˆí•œ ê¸´ ë¬¸ë‹¨ ì—†ìŒ. ë§ˆì§€ë§‰ ë¬¸ë‹¨ì— ì¶”ê°€ ì‹œë„.")
            if paragraphs:
                idx_to_add = len(paragraphs) -1
            else:
                paragraphs.append("")
                idx_to_add = 0
            
            if len(paragraphs[idx_to_add]) < 50 :
                 paragraphs[idx_to_add] += self._generate_sentences_with_morpheme(morpheme, count_to_add)
            else:
                 paragraphs[idx_to_add] = self._inject_morpheme_into_paragraph(paragraphs[idx_to_add], morpheme, count_to_add)
            return "\n\n".join(paragraphs)

        add_counts_per_paragraph = {idx: 0 for idx in normal_paragraphs_indices}
        for i in range(count_to_add):
            idx_to_add = normal_paragraphs_indices[i % len(normal_paragraphs_indices)]
            add_counts_per_paragraph[idx_to_add] += 1
            
        for idx, num_to_add_in_para in add_counts_per_paragraph.items():
            if num_to_add_in_para > 0:
                paragraphs[idx] = self._inject_morpheme_into_paragraph(paragraphs[idx], morpheme, num_to_add_in_para)
        
        return "\n\n".join(paragraphs)

    def _generate_sentences_with_morpheme(self, morpheme, count):
        """ í˜•íƒœì†Œê°€ í¬í•¨ëœ ë¬¸ì¥ ìƒì„± (ê°„ë‹¨ ë²„ì „) """
        sentences = []
        templates = [
            f"ë˜í•œ, {morpheme}ì˜ ì¤‘ìš”ì„±ì„ ê°„ê³¼í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.",
            f"ì´ëŸ¬í•œ ë§¥ë½ì—ì„œ {morpheme}ì€ í•µì‹¬ì ì¸ ì—­í• ì„ í•©ë‹ˆë‹¤.",
            f"ê²°ê³¼ì ìœ¼ë¡œ {morpheme}ì˜ í™œìš©ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.",
            f"ë§ì€ ì „ë¬¸ê°€ë“¤ì´ {morpheme}ì˜ ê°€ì¹˜ë¥¼ ê°•ì¡°í•©ë‹ˆë‹¤.",
            f"íŠ¹íˆ {morpheme}ì— ëŒ€í•œ ì´í•´ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        ]
        for _ in range(count):
            sentences.append(random.choice(templates))
        return " ".join(sentences)

    def _inject_morpheme_into_paragraph(self, paragraph, morpheme, count_to_add):
        """ ê¸°ì¡´ ë¬¸ë‹¨ì— í˜•íƒœì†Œë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì‚½ì… """
        sentences = re.split(r'(?<=[.!?])\s+', paragraph.strip())
        if not sentences or not sentences[0]:
            return paragraph + " " + self._generate_sentences_with_morpheme(morpheme, count_to_add)

        for _ in range(count_to_add):
            insert_idx = random.randrange(len(sentences) + 1)
            prefix_phrases = ["ë§ë¶™ì—¬ ë§í•˜ìë©´, ", "ì¤‘ìš”í•œ ì ì€ ", "ì˜ˆë¥¼ ë“¤ì–´, "]
            suffix_phrases = [f" ì—­ì‹œ ì¤‘ìš”í•©ë‹ˆë‹¤.", f"ë„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.", f"ì˜ í™œìš©ë„ ìƒê°í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."]
            
            new_phrase_templates = [
                f"{random.choice(prefix_phrases)}{morpheme}ì˜ ê²½ìš°",
                f"{morpheme}{random.choice(suffix_phrases)}",
                f"{morpheme} ê´€ë ¨í•˜ì—¬"
            ]
            new_phrase = random.choice(new_phrase_templates)

            if insert_idx == len(sentences):
                sentences[-1] = sentences[-1].rstrip('.!?') + f", íŠ¹íˆ {morpheme}ì˜ ì¤‘ìš”ì„±ì´ ë¶€ê°ë©ë‹ˆë‹¤."
            elif insert_idx == 0:
                sentences[0] = f"{morpheme}ì— ëŒ€í•´ ë§í•˜ìë©´, " + sentences[0]
            else:
                sentences.insert(insert_idx, new_phrase)
        
        return " ".join(s.strip() for s in sentences if s.strip())

    def _ask_llm_for_sentence_reduction(self, sentence, morpheme_to_reduce):
        """
        Claudeì—ê²Œ íŠ¹ì • í˜•íƒœì†Œë¥¼ ë¬¸ì¥ì—ì„œ ì œê±°í•˜ê±°ë‚˜ ë¬¸ì¥ ì „ì²´ë¥¼ ì‚­ì œí• ì§€ ë¬¸ì˜í•˜ê³ ,
        ìì—°ìŠ¤ëŸ¬ì›€ì„ ìœ ì§€í•˜ë„ë¡ ìš”ì²­í•©ë‹ˆë‹¤.
        """
        prompt = f"""
        ë‹¹ì‹ ì€ ì „ë¬¸ ì½˜í…ì¸  í¸ì§‘ìì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì¥ì—ì„œ íŠ¹ì • ë‹¨ì–´/êµ¬ë¬¸ì˜ ì¶œí˜„ì„ ì¤„ì´ë©´ì„œ
        ë¬¸ì¥ì˜ ìì—°ìŠ¤ëŸ¬ì›€ê³¼ ì˜ë¯¸ë¥¼ ìœ ì§€í•˜ëŠ” ê²ƒì´ ë‹¹ì‹ ì˜ ì„ë¬´ì…ë‹ˆë‹¤.

        ì¤„ì—¬ì•¼ í•  ë‹¨ì–´/êµ¬ë¬¸: "{morpheme_to_reduce}"
        ë¬¸ì¥: "{sentence}"

        ë¬¸ì¥ì„ ë¶„ì„í•˜ê³  ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ê²°ì •í•˜ì„¸ìš”:
        1. ë‹¨ì–´/êµ¬ë¬¸ "{morpheme_to_reduce}"ë¥¼ ë¬¸ì¥ì—ì„œ ì œê±°í•´ë„ ë¬¸ì¥ì´ ë¶€ìì—°ìŠ¤ëŸ¬ì›Œì§€ê±°ë‚˜
           í•„ìˆ˜ì ì¸ ì˜ë¯¸ë¥¼ ìƒì§€ ì•ŠìŠµë‹ˆê¹Œ? ê·¸ë ‡ë‹¤ë©´ ìˆ˜ì •ëœ ë¬¸ì¥ì„ ì œê³µí•˜ì„¸ìš”.
        2. ë‹¨ì–´/êµ¬ë¬¸ë§Œ ì œê±°í•˜ë©´ ë¬¸ì¥ì´ ë¶€ìì—°ìŠ¤ëŸ¬ì›Œì§€ê±°ë‚˜ ì˜ë¯¸ê°€ í¬ê²Œ ë³€í•©ë‹ˆê¹Œ?
           ê·¸ë ‡ë‹¤ë©´ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ì˜ ì „ì²´ì ì¸ íë¦„ê³¼ ì¼ê´€ì„±ì— ì˜í–¥ì„ ì£¼ì§€ ì•Šê³ 
           ë¬¸ì¥ ì „ì²´ë¥¼ ì œê±°í•  ìˆ˜ ìˆìŠµë‹ˆê¹Œ? ê·¸ë ‡ë‹¤ë©´ ë¹ˆ ë¬¸ìì—´ì„ ì¶œë ¥í•˜ì„¸ìš”.
        3. ìœ„ ë‘ ê°€ì§€ ëª¨ë‘ ë¶ˆê°€ëŠ¥í•œ ê²½ìš° (ì¦‰, ë‹¨ì–´/êµ¬ë¬¸ì´ í•„ìˆ˜ì ì´ê³  ë¬¸ì¥ì„ ì œê±°í•  ìˆ˜ ì—†ëŠ” ê²½ìš°),
           ì›ë˜ ë¬¸ì¥ì„ ë³€ê²½í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì„¸ìš”.

        ìˆ˜ì •ëœ ë¬¸ì¥ë§Œ ì¶œë ¥í•˜ê±°ë‚˜, ë¬¸ì¥ì„ ì œê±°í•´ì•¼ í•œë‹¤ë©´ ë¹ˆ ë¬¸ìì—´ì„ ì¶œë ¥í•˜ì„¸ìš”.
        ì–´ë–¤ ì„¤ëª…ì´ë‚˜ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ë„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.
        """
        try:
            response = self.model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.3,
                    max_output_tokens=1024
                )
            )
            return response.text.strip()
        except Exception as e:
            logger.error(f"Gemini sentence reduction API error: {e}")
            return sentence

    def _reduce_morpheme_to_target(self, content, morpheme_to_reduce, target_count, all_target_morphemes_dict):
        """
        íŠ¹ì • í˜•íƒœì†Œì˜ ì¶œí˜„ íšŸìˆ˜ë¥¼ ëª©í‘œì¹˜(target_count)ê¹Œì§€ ì¤„ì…ë‹ˆë‹¤.
        Geminiì—ê²Œ ë¬¸ë§¥ìƒ ìì—°ìŠ¤ëŸ¬ì›€ì„ í™•ì¸í•˜ë„ë¡ ìš”ì²­í•©ë‹ˆë‹¤.
        """
        logger.info(f"í˜•íƒœì†Œ '{morpheme_to_reduce}' íšŸìˆ˜ë¥¼ ëª©í‘œì¹˜({target_count}íšŒ)ì— ë§ê²Œ ì œê±° (Gemini ë¬¸ë§¥ ê³ ë ¤)")

        # Determine counting method based on morpheme type (base or compound)
        temp_analysis_for_type = self.morpheme_analyzer.analyze(content, "", all_target_morphemes_dict['all_list'])
        morpheme_type = temp_analysis_for_type['morpheme_analysis']['counts'].get(morpheme_to_reduce, {}).get('type', 'base')

        if morpheme_type == 'base':
            count_func = self.morpheme_analyzer._count_substring
            pattern = re.escape(morpheme_to_reduce) # Substring pattern for finding sentences
        else: # compound
            count_func = self.morpheme_analyzer._count_exact_word
            if re.search(r'[ê°€-í£]', morpheme_to_reduce):
                if ' ' in morpheme_to_reduce:
                    pattern = re.escape(morpheme_to_reduce)
                else:
                    pattern = rf'(?<![ê°€-í£]){re.escape(morpheme_to_reduce)}(?![ê°€-í£])'
            else:
                pattern = rf'\b{re.escape(morpheme_to_reduce)}\b'
        
        current_content = content
        previous_content = ""
        attempt = 0
        max_attempts = 30 # Safety break for infinite loop

        while attempt < max_attempts:
            if current_content == previous_content:
                logger.warning(f"'{morpheme_to_reduce}' ê°ì†Œ ê³¼ì •ì´ ê³ ì°© ìƒíƒœì…ë‹ˆë‹¤. ë£¨í”„ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break
            previous_content = current_content

            current_count = count_func(morpheme_to_reduce, current_content)
            
            if current_count <= target_count:
                logger.info(f"í˜•íƒœì†Œ '{morpheme_to_reduce}' ëª©í‘œì¹˜({target_count}íšŒ) ë‹¬ì„± (í˜„ì¬ {current_count}íšŒ).")
                return current_content

            sentences = re.split(r'(?<=[.!?])\s+', current_content)
            
            sentences_with_morpheme_indices = []
            for i, s in enumerate(sentences):
                if re.search(pattern, s): # Use the correct pattern for finding sentences containing the morpheme
                    sentences_with_morpheme_indices.append(i)
            
            if not sentences_with_morpheme_indices:
                logger.warning(f"í˜•íƒœì†Œ '{morpheme_to_reduce}'ë¥¼ í¬í•¨í•˜ëŠ” ë¬¸ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (í˜„ì¬ {current_count}íšŒ)")
                break

            # Send ALL relevant sentences to Gemini for processing
            modified_sentences_map = {}
            for idx in sentences_with_morpheme_indices:
                original_sentence = sentences[idx]
                reduced_sentence_or_keyword = self._ask_llm_for_sentence_reduction(original_sentence, morpheme_to_reduce)
                modified_sentences_map[idx] = reduced_sentence_or_keyword
                
                if reduced_sentence_or_keyword != original_sentence:
                    logger.info(f"Gemini: ë¬¸ì¥ '{original_sentence[:30]}...'ì—ì„œ í˜•íƒœì†Œ '{morpheme_to_reduce}' ìˆ˜ì •/ì œê±° ì‹œë„.")
                else:
                    logger.info(f"Gemini: ë¬¸ì¥ '{original_sentence[:30]}...' ë³€ê²½ ì—†ìŒ.")

            new_sentences = [modified_sentences_map.get(i, s) for i, s in enumerate(sentences)]
            
            current_content = " ".join(s for s in new_sentences if s) # Filter out empty strings from deleted sentences
            
            updated_count = count_func(morpheme_to_reduce, current_content)
            logger.info(f"í˜•íƒœì†Œ '{morpheme_to_reduce}' ì œê±° ì‹œë„ #{attempt+1}. í˜„ì¬ íšŸìˆ˜: {updated_count}")
            attempt += 1

        logger.warning(f"í˜•íƒœì†Œ '{morpheme_to_reduce}' {max_attempts}íšŒ ì‹œë„ í›„ì—ë„ ëª©í‘œì¹˜({target_count}íšŒ) ë¯¸ë‹¬ì„±. í˜„ì¬ {count_func(morpheme_to_reduce, current_content)}íšŒ.")
        return current_content

    def _get_enhanced_substitutions(self, morpheme):
        substitutions = self.substitution_generator.get_substitutions(morpheme)
        if len(substitutions) < 3:
            default_subs = ["ì´ê²ƒ", "ê·¸ê²ƒ", "í•´ë‹¹ ë‚´ìš©", "ì´ ë¶€ë¶„", "ê´€ë ¨ëœ ê²ƒ"]
            if len(morpheme) > 3:
                default_subs.append("") 
            substitutions.extend(s for s in default_subs if s not in substitutions)
        return list(set(substitutions))

    def _enforce_exact_char_count_v2(self, content, target_char_count, tolerance=50, all_target_morphemes=None, current_morpheme_counts=None):
        current_char_count = len(content.replace(" ", ""))
        min_chars = target_char_count - tolerance
        max_chars = target_char_count + tolerance

        if min_chars <= current_char_count <= max_chars:
            return content
            
        paragraphs = re.split(r'(\n\n+)', content)
        
        processed_paragraphs = []
        temp_para = ""
        for part in paragraphs:
            if part == "\n\n" or part == "\n":
                if temp_para:
                    processed_paragraphs.append(temp_para)
                    temp_para = ""
                processed_paragraphs.append(part)
            else:
                temp_para += part
        if temp_para:
            processed_paragraphs.append(temp_para)

        content_paragraphs_with_indices = []
        for i, p_text in enumerate(processed_paragraphs):
            if not (p_text == "\n\n" or p_text == "\n") and p_text.strip():
                 if not p_text.strip().startswith(('#', '##', '###')):
                    content_paragraphs_with_indices.append({'original_idx': i, 'text': p_text, 'len': len(p_text.replace(" ",""))})
        
        if not content_paragraphs_with_indices:
            logger.warning("ê¸€ììˆ˜ ì¡°ì •: ìˆ˜ì •í•  ë‚´ìš© ë¬¸ë‹¨ ì—†ìŒ.")
            return content

        if current_char_count < min_chars:
            chars_to_add = min_chars - current_char_count
            logger.info(f"ê¸€ììˆ˜ ì¡°ì •: {chars_to_add}ì ì¶”ê°€ í•„ìš”")
            
            content_paragraphs_with_indices.sort(key=lambda x: x['len'])
            
            added_chars_total = 0
            for para_info in content_paragraphs_with_indices:
                if added_chars_total >= chars_to_add: break
                
                current_para_add = (chars_to_add - added_chars_total) // (len(content_paragraphs_with_indices) - content_paragraphs_with_indices.index(para_info)) if len(content_paragraphs_with_indices) > content_paragraphs_with_indices.index(para_info) else (chars_to_add - added_chars_total)
                current_para_add = max(20, current_para_add)
                
                expanded_text = self._expand_paragraph(para_info['text'], current_para_add, all_target_morphemes, current_morpheme_counts)
                char_diff = len(expanded_text.replace(" ","")) - para_info['len']
                processed_paragraphs[para_info['original_idx']] = expanded_text
                added_chars_total += char_diff
                if added_chars_total >= chars_to_add: break
            
        elif current_char_count > max_chars:
            chars_to_remove = current_char_count - max_chars
            logger.info(f"ê¸€ììˆ˜ ì¡°ì •: {chars_to_remove}ì ì œê±° í•„ìš”")

            content_paragraphs_with_indices.sort(key=lambda x: x['len'], reverse=True)
            removed_chars_total = 0
            for para_info in content_paragraphs_with_indices:
                if removed_chars_total >= chars_to_remove: break
                if para_info['len'] < 50 : continue

                current_para_remove = min(
                    (chars_to_remove - removed_chars_total) // (len(content_paragraphs_with_indices) - content_paragraphs_with_indices.index(para_info) if len(content_paragraphs_with_indices) > content_paragraphs_with_indices.index(para_info) else 1),
                    para_info['len'] // 3
                )
                current_para_remove = max(20, current_para_remove)

                if current_para_remove > 0:
                    reduced_text = self._reduce_paragraph(para_info['text'], current_para_remove, all_target_morphemes, current_morpheme_counts)
                    char_diff = para_info['len'] - len(reduced_text.replace(" ",""))
                    processed_paragraphs[para_info['original_idx']] = reduced_text
                    removed_chars_total += char_diff
                    if removed_chars_total >= chars_to_remove: break
            
        return "".join(processed_paragraphs)

    def _expand_paragraph(self, paragraph, chars_to_add, all_target_morphemes_dict, current_morpheme_counts):
        """
        ë¬¸ë‹¨ì„ í™•ì¥í•˜ì—¬ ê¸€ììˆ˜ë¥¼ ëŠ˜ë¦½ë‹ˆë‹¤.
        ê³¼ë‹¤í•˜ê²Œ ì¶œí˜„í•˜ëŠ” ëª©í‘œ í˜•íƒœì†Œê°€ ì¬ìœ ì…ë˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•©ë‹ˆë‹¤.
        """
        if chars_to_add <=0: return paragraph
        
        sentences = re.split(r'(?<=[.!?])\s+', paragraph.strip())
        last_sentence = sentences[-1] if sentences and sentences[-1] else ""
        
        try:
            nouns = self.okt.nouns(last_sentence if last_sentence else paragraph)
            key_phrases = [n for n in nouns if len(n) > 1][:3]
        except Exception:
            key_phrases = ["ì´ ì£¼ì œ", "ê´€ë ¨ ë‚´ìš©"]

        filtered_key_phrases = []
        if all_target_morphemes_dict and current_morpheme_counts:
            for phrase in key_phrases:
                is_over_represented = False
                if phrase in all_target_morphemes_dict['all_list']:
                    morpheme_info = current_morpheme_counts.get(phrase, {})
                    morpheme_type = morpheme_info.get('type')
                    count = morpheme_info.get('count', 0)
                    
                    if morpheme_type == 'base' and count >= self.morpheme_analyzer.target_max_base_count:
                        is_over_represented = True
                    elif morpheme_type == 'compound' and count >= self.morpheme_analyzer.target_max_compound_count:
                        is_over_represented = True
                
                if not is_over_represented:
                    filtered_key_phrases.append(phrase)
        else:
            filtered_key_phrases = key_phrases
        
        if not filtered_key_phrases:
            filtered_key_phrases = ["ì´ ì ", "ì´ ë¶€ë¶„", "í•´ë‹¹ ë‚´ìš©"]

        expansion_text = ""
        added_len = 0
        templates = [
            " ì´ì— ë”í•´, {phrase}ì— ëŒ€í•œ ì‹¬ì¸µì ì¸ ì´í•´ê°€ í•„ìš”í•©ë‹ˆë‹¤.",
            " ë˜í•œ {phrase}ì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤.",
            " {phrase}ì™€ ê´€ë ¨í•˜ì—¬ ì¶”ê°€ì ì¸ ì •ë³´ë¥¼ ì œê³µí•˜ìë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.",
            " ì‹¤ì œë¡œ {phrase}ëŠ” ë§ì€ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.",
            " ê·¸ë¦¬ê³  {phrase}ì— ëŒ€í•œ ê³ ë ¤ë„ ì¤‘ìš”í•©ë‹ˆë‹¤."
        ]
        while added_len < chars_to_add:
            chosen_phrase = random.choice(filtered_key_phrases)
            sentence_to_add = random.choice(templates).format(phrase=chosen_phrase)
            
            contains_over_represented_in_new_sentence = False
            if all_target_morphemes_dict and current_morpheme_counts:
                temp_analysis = self.morpheme_analyzer.analyze(sentence_to_add, "", all_target_morphemes_dict['all_list'])
                for morpheme in all_target_morphemes_dict['all_list']:
                    morpheme_info_in_new_sentence = temp_analysis['morpheme_analysis']['counts'].get(morpheme, {})
                    count_in_new_sentence = morpheme_info_in_new_sentence.get('count', 0)
                    morpheme_type = morpheme_info_in_new_sentence.get('type')

                    if count_in_new_sentence > 0:
                        current_global_count = current_morpheme_counts.get(morpheme, {}).get('count', 0)
                        
                        if morpheme_type == 'base' and current_global_count >= self.morpheme_analyzer.target_max_base_count:
                            contains_over_represented_in_new_sentence = True
                            break
                        elif morpheme_type == 'compound' and current_global_count >= self.morpheme_analyzer.target_max_compound_count:
                            contains_over_represented_in_new_sentence = True
                            break
            
            if not contains_over_represented_in_new_sentence:
                expansion_text += sentence_to_add
                added_len += len(sentence_to_add.replace(" ", ""))
            
            if len(expansion_text) > chars_to_add * 1.5 : break

        return paragraph + expansion_text

    def _reduce_paragraph(self, paragraph, chars_to_remove, all_target_morphemes_dict, current_morpheme_counts):
        if chars_to_remove <= 0: return paragraph

        sentences = re.split(r'(?<=[.!?])\s+', paragraph.strip())
        if len(sentences) <= 1:
            words = paragraph.split()
            reduced_len = 0
            while reduced_len < chars_to_remove and len(words) > 5:
                removed_word = words.pop()
                reduced_len += len(removed_word.replace(" ",""))
            return " ".join(words) + ("." if paragraph.endswith(".") else "")

        sentence_info = []
        for i, s in enumerate(sentences):
            score = 100 - len(s)
            if any(conj in s for conj in ["í•˜ì§€ë§Œ", "ê·¸ëŸ¬ë‚˜", "ë”°ë¼ì„œ", "ê²°ë¡ ì ìœ¼ë¡œ"]):
                score -= 50
            
            if all_target_morphemes_dict and current_morpheme_counts:
                temp_analysis = self.morpheme_analyzer.analyze(s, "", all_target_morphemes_dict['all_list'])
                for morpheme in all_target_morphemes_dict['all_list']:
                    morpheme_info_in_sentence = temp_analysis['morpheme_analysis']['counts'].get(morpheme, {})
                    count_in_sentence = morpheme_info_in_sentence.get('count', 0)
                    morpheme_type = morpheme_info_in_sentence.get('type')

                    if count_in_sentence > 0:
                        current_global_count = current_morpheme_counts.get(morpheme, {}).get('count', 0)
                        
                        if morpheme_type == 'base' and current_global_count > self.morpheme_analyzer.target_max_base_count:
                            score += 200
                            break
                        elif morpheme_type == 'compound' and current_global_count > self.morpheme_analyzer.target_max_compound_count:
                            score += 150
                            break

            sentence_info.append({'idx': i, 'text': s, 'score': score, 'len': len(s.replace(" ",""))})
        
        sentence_info.sort(key=lambda x: x['score'], reverse=True)

        removed_chars_count = 0
        removed_indices = set()
        new_sentences = list(sentences)

        for s_info in sentence_info:
            if removed_chars_count >= chars_to_remove: break
            if len(sentences) - len(removed_indices) <= 1 : break 

            if s_info['idx'] not in removed_indices:
                original_sentence = s_info['text']
                reconstructed_sentence = original_sentence
                
                patterns_to_remove = [
                    (r"ë§¤ìš°\s+", ""), (r"ì •ë§\s+", ""), (r"ì•„ì£¼\s+", ""),
                    (r"í•˜ëŠ”\s+ê²ƒì€", ""), (r"ì—\s+ëŒ€í•˜ì—¬", ""), (r"ì—\s+ê´€í•œ", ""),
                    (r"ì´ë¼ê³ \s+í• \s+ìˆ˜\s+ìˆë‹¤", ""), (r"ë¼ê³ \s+ë³¼\s+ìˆ˜\s+ìˆë‹¤", ""),
                    (r"~ì—\s+ë”°ë¥´ë©´", ""),
                    (r"~ì˜\s+ê²½ìš°", ""),
                ]
                for pattern, replacement in patterns_to_remove:
                    reconstructed_sentence = re.sub(pattern, replacement, reconstructed_sentence)

                words = reconstructed_sentence.split()
                temp_words = []
                for word in words:
                    if word in all_target_morphemes_dict['all_list']:
                        temp_words.append(word)
                        continue
                    
                    substitutions = self.substitution_generator.get_substitutions(word)
                    safe_subs = [
                        sub for sub in substitutions
                        if not any(target_m in sub for target_m in all_target_morphemes_dict['all_list']) and len(sub) < len(word)
                    ]
                    if safe_subs and random.random() < 0.3:
                        new_word = random.choice(safe_subs)
                        temp_words.append(new_word)
                        removed_chars_count += len(word.replace(" ", "")) - len(new_word.replace(" ", ""))
                        logger.debug(f"ë‹¨ì–´ ëŒ€ì²´: '{word}' -> '{new_word}'")
                    else:
                        temp_words.append(word)
                reconstructed_sentence = " ".join(temp_words)

                char_diff_from_reconstruction = len(original_sentence.replace(" ", "")) - len(reconstructed_sentence.replace(" ", ""))
                if char_diff_from_reconstruction > 0:
                    new_sentences[s_info['idx']] = reconstructed_sentence
                    removed_chars_count += char_diff_from_reconstruction
                    logger.debug(f"ë¬¸ì¥ ì¬êµ¬ì„±: '{original_sentence}' -> '{reconstructed_sentence}' (ì¤„ì–´ë“  ê¸€ììˆ˜: {char_diff_from_reconstruction})")
                else:
                    new_sentences[s_info['idx']] = original_sentence
                
                if s_info['idx'] not in removed_indices and (new_sentences[s_info['idx']] == original_sentence or char_diff_from_reconstruction < 5):
                    if s_info['len'] > 20 and removed_chars_count + s_info['len'] <= chars_to_remove:
                        removed_chars_count += s_info['len']
                        removed_indices.add(s_info['idx'])
                        logger.debug(f"ë¬¸ì¥ ì‚­ì œ: '{s_info['text']}'")
        
        final_sentences = [new_sentences[i] for i in range(len(new_sentences)) if i not in removed_indices]
        return " ".join(final_sentences)

    def _enforce_exact_target_morpheme_count(self, content, keyword, custom_morphemes, current_morpheme_counts, target_morphemes_dict):
        """
        'ëª©í‘œ' í˜•íƒœì†Œ ì¶œí˜„ íšŸìˆ˜ë¥¼ ëª©í‘œ ë²”ìœ„ ë‚´ë¡œ ì¡°ì • (MorphemeAnalyzer ì‚¬ìš©)
        target_morphemes_dict now contains 'base' and 'compound' lists.
        """
        adjusted_content = content
        
        base_morphemes = target_morphemes_dict['base']
        compound_morphemes = target_morphemes_dict['compound']
        all_target_morphemes_list = target_morphemes_dict['all_list']

        # Adjust base morphemes first
        for morpheme in base_morphemes:
            current_count_for_morpheme = self.morpheme_analyzer._count_substring(morpheme, adjusted_content)
            
            target_min = self.morpheme_analyzer.target_min_base_count
            target_max = self.morpheme_analyzer.target_max_base_count

            if current_count_for_morpheme > target_max:
                target_count = (target_min + target_max) // 2
                logger.info(f"í•µì‹¬ ê¸°ë³¸ í˜•íƒœì†Œ '{morpheme}' ê³¼ë‹¤: {current_count_for_morpheme}íšŒ -> ëª©í‘œ {target_count}íšŒë¡œ ì¤„ì„")
                adjusted_content = self._reduce_morpheme_to_target(
                    adjusted_content, 
                    morpheme, 
                    target_count, 
                    target_morphemes_dict # Pass the full dict
                )
            elif current_count_for_morpheme < target_min:
                shortage = target_min - current_count_for_morpheme
                logger.info(f"í•µì‹¬ ê¸°ë³¸ í˜•íƒœì†Œ '{morpheme}' ë¶€ì¡±: {current_count_for_morpheme}íšŒ -> {target_min}íšŒë¡œ ëŠ˜ë¦¼ (ì¶”ê°€ëŸ‰: {shortage}íšŒ)")
                adjusted_content = self._add_morpheme_strategically(adjusted_content, morpheme, shortage)

        # Adjust compound morphemes next
        for morpheme in compound_morphemes:
            current_count_for_morpheme = self.morpheme_analyzer._count_exact_word(morpheme, adjusted_content)
            
            target_min = self.morpheme_analyzer.target_min_compound_count
            target_max = self.morpheme_analyzer.target_max_compound_count

            if current_count_for_morpheme > target_max:
                target_count = (target_min + target_max) // 2
                logger.info(f"ë³µí•© í‚¤ì›Œë“œ/êµ¬ë¬¸ '{morpheme}' ê³¼ë‹¤: {current_count_for_morpheme}íšŒ -> ëª©í‘œ {target_count}íšŒë¡œ ì¤„ì„")
                adjusted_content = self._reduce_morpheme_to_target(
                    adjusted_content, 
                    morpheme, 
                    target_count, 
                    target_morphemes_dict # Pass the full dict
                )
            elif current_count_for_morpheme < target_min:
                shortage = target_min - current_count_for_morpheme
                logger.info(f"ë³µí•© í‚¤ì›Œë“œ/êµ¬ë¬¸ '{morpheme}' ë¶€ì¡±: {current_count_for_morpheme}íšŒ -> {target_min}íšŒë¡œ ëŠ˜ë¦¼ (ì¶”ê°€ëŸ‰: {shortage}íšŒ)")
                adjusted_content = self._add_morpheme_strategically(adjusted_content, morpheme, shortage)
        
        return adjusted_content

    def _add_morpheme_naturally(self, content, morpheme, count_to_add):
        return self._add_morpheme_strategically(content, morpheme, count_to_add)

    def separate_content_and_refs(self, content):
        refs_pattern = r"(## ì°¸ê³ ìë£Œ[\s\S]*)"
        refs_match = re.search(refs_pattern, content, re.MULTILINE)
        
        if refs_match:
            refs_section = refs_match.group(1)
            content_without_refs = content[:refs_match.start()].strip()
            return {
                'content_without_refs': content_without_refs,
                'refs_section': refs_section
            }
        else:
            return {
                'content_without_refs': content.strip(),
                'refs_section': None
            }
    
    def _create_seo_optimization_prompt(self, content, keyword, custom_morphemes, analysis_result):
        char_count = analysis_result['char_count']
        target_min_chars = self.morpheme_analyzer.target_min_chars
        target_max_chars = self.morpheme_analyzer.target_max_chars
        char_count_direction = ""
        if char_count < target_min_chars:
            char_count_direction = f"í˜„ì¬ {char_count}ì. {target_min_chars - char_count}ì ì´ìƒ ëŠ˜ë ¤ {target_min_chars}-{target_max_chars}ì ë²”ìœ„ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”."
        elif char_count > target_max_chars:
            char_count_direction = f"í˜„ì¬ {char_count}ì. {char_count - target_max_chars}ì ì´ìƒ ì¤„ì—¬ {target_min_chars}-{target_max_chars}ì ë²”ìœ„ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”."
        else:
            char_count_direction = f"í˜„ì¬ ê¸€ììˆ˜ {char_count}ìëŠ” ì ì ˆí•œ ë²”ìœ„({target_min_chars}-{target_max_chars}ì)ì…ë‹ˆë‹¤. ì´ ë²”ìœ„ë¥¼ ìœ ì§€í•´ì£¼ì„¸ìš”."

        morpheme_issues = []
        
        base_morphemes = analysis_result['morpheme_analysis']['target_morphemes']['base']
        compound_morphemes = analysis_result['morpheme_analysis']['target_morphemes']['compound']
        current_counts = analysis_result['morpheme_analysis']['counts']

        target_min_base_morph = self.morpheme_analyzer.target_min_base_count
        target_max_base_morph = self.morpheme_analyzer.target_max_base_count
        target_min_compound_morph = self.morpheme_analyzer.target_min_compound_count
        target_max_compound_morph = self.morpheme_analyzer.target_max_compound_count

        for morpheme in base_morphemes:
            info = current_counts.get(morpheme, {})
            count = info.get('count', 0)
            if not info.get('is_valid', True):
                morpheme_issues.append(f"â€¢ í•µì‹¬ ê¸°ë³¸ í˜•íƒœì†Œ '{morpheme}': í˜„ì¬ {count}íšŒ â†’ ëª©í‘œ {target_min_base_morph}-{target_max_base_morph}íšŒ (ë¶€ì¡±/ê³¼ë‹¤)")
        
        for morpheme in compound_morphemes:
            info = current_counts.get(morpheme, {})
            count = info.get('count', 0)
            if not info.get('is_valid', True):
                morpheme_issues.append(f"â€¢ ë³µí•© í‚¤ì›Œë“œ/êµ¬ë¬¸ '{morpheme}': í˜„ì¬ {count}íšŒ â†’ ëª©í‘œ {target_min_compound_morph}-{target_max_compound_morph}íšŒ (ë¶€ì¡±/ê³¼ë‹¤)")
        
        morpheme_text = "\n".join(morpheme_issues) if morpheme_issues else "ëª¨ë“  ëª©í‘œ í˜•íƒœì†Œê°€ ì ì • ë²”ìœ„ ë‚´ì— ìˆìŠµë‹ˆë‹¤."
        
        return f"""
        ì´ ë¸”ë¡œê·¸ ì½˜í…ì¸ ë¥¼ SEOì™€ ê°€ë…ì„± ì¸¡ë©´ì—ì„œ ìµœì í™”í•´ì£¼ì„¸ìš”. ì•„ë˜ ìš”êµ¬ì‚¬í•­ì„ ì¶©ì¡±í•˜ë©´ì„œ ì‚¬ìš©ì ê²½í—˜ì„ ê°œì„ í•´ì•¼ í•©ë‹ˆë‹¤:

        1ï¸âƒ£ ê¸€ììˆ˜ ìš”êµ¬ì‚¬í•­: {target_min_chars}-{target_max_chars}ì (ê³µë°± ì œì™¸)
        {char_count_direction}

        2ï¸âƒ£ í‚¤ì›Œë“œ ë° ì£¼ìš” í˜•íƒœì†Œ ìµœì í™”:
        {morpheme_text}

        3ï¸âƒ£ í‚¤ì›Œë“œ ë° í˜•íƒœì†Œ ì¹´ìš´íŒ… ë°©ì‹:
           - í•µì‹¬ ê¸°ë³¸ í˜•íƒœì†Œ (ì˜ˆ: 'ì—”ì§„', 'ì˜¤ì¼', 'ì¢…ë¥˜'): ë¬¸ì¥ ë‚´ì—ì„œ ë¶€ë¶„ì ìœ¼ë¡œ í¬í•¨ë˜ì–´ë„ ì¹´ìš´íŠ¸ë©ë‹ˆë‹¤. (ì˜ˆ: 'ì—”ì§„ì˜¤ì¼ì¢…ë¥˜'ì—ì„œ 'ì—”ì§„' 1íšŒ, 'ì˜¤ì¼' 1íšŒ, 'ì¢…ë¥˜' 1íšŒ)
           - ë³µí•© í‚¤ì›Œë“œ ë° êµ¬ë¬¸ (ì˜ˆ: 'ì—”ì§„ì˜¤ì¼', 'ì—”ì§„ì˜¤ì¼ì¢…ë¥˜'): ì •í™•íˆ í•´ë‹¹ êµ¬ë¬¸ì´ ì¼ì¹˜í•´ì•¼ ì¹´ìš´íŠ¸ë©ë‹ˆë‹¤.

        4ï¸âƒ£ SEO ìµœì í™” ì „ëµ:
        â€¢ ì²« ë²ˆì§¸ ë¬¸ë‹¨ì— í•µì‹¬ í‚¤ì›Œë“œ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨
        â€¢ ì£¼ìš” ì†Œì œëª©ì— í‚¤ì›Œë“œ ê´€ë ¨ ë¬¸êµ¬ í¬í•¨
        â€¢ ì§§ê³  ê°„ê²°í•œ ë¬¸ë‹¨ ì‚¬ìš© (2-3ë¬¸ì¥ ê¶Œì¥)
        â€¢ í•µì‹¬ í‚¤ì›Œë“œì˜ ìì—°ìŠ¤ëŸ¬ìš´ ë¶„í¬
        â€¢ ëª…í™•í•œ ë¬¸ë‹¨ êµ¬ë¶„ê³¼ ì†Œì œëª© í™œìš©
        â€¢ ëª¨ë°”ì¼ ì¹œí™”ì ì¸ ì§§ì€ ë¬¸ì¥ ì‚¬ìš©

        5ï¸âƒ£ ì‚¬ìš©ì ê²½í—˜ ê°œì„ :
        â€¢ ê¸€ë¨¸ë¦¬ ê¸°í˜¸ë‚˜ ë²ˆí˜¸ ë§¤ê¸°ê¸°ë¡œ ë‚´ìš© êµ¬ì¡°í™”
        â€¢ í•µì‹¬ ì •ë³´ë¥¼ ë¨¼ì € ì œì‹œí•˜ëŠ” ì—­í”¼ë¼ë¯¸ë“œ êµ¬ì¡°
        â€¢ ì „ë¬¸ ìš©ì–´ëŠ” ì ì ˆí•œ ì„¤ëª…ê³¼ í•¨ê»˜ ì‚¬ìš©
        â€¢ ì§ê´€ì ì´ê³  ëª…í™•í•œ í‘œí˜„ ì‚¬ìš©

        ì›ë³¸ ì½˜í…ì¸ :
        {content}

        ìµœì í™”ëœ ë‚´ìš©ë§Œ ì œê³µí•´ ì£¼ì„¸ìš”. ì„¤ëª…ì´ë‚˜ ë©”ëª¨ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
        """
    
    def _create_seo_readability_prompt(self, content, keyword, custom_morphemes, analysis_result):
        base_morphemes = analysis_result['morpheme_analysis']['target_morphemes']['base']
        compound_morphemes = analysis_result['morpheme_analysis']['target_morphemes']['compound']
        
        target_min_base_morph = self.morpheme_analyzer.target_min_base_count
        target_max_base_morph = self.morpheme_analyzer.target_max_base_count
        target_min_compound_morph = self.morpheme_analyzer.target_min_compound_count
        target_max_compound_morph = self.morpheme_analyzer.target_max_compound_count

        morpheme_instructions = []
        if base_morphemes:
            morpheme_instructions.append(f"í•µì‹¬ ê¸°ë³¸ í˜•íƒœì†Œ ({', '.join(base_morphemes)}): ê°ê° {target_min_base_morph}-{target_max_base_morph}íšŒ")
        if compound_morphemes:
            morpheme_instructions.append(f"ë³µí•© í‚¤ì›Œë“œ ë° êµ¬ë¬¸ ({', '.join(compound_morphemes)}): ê°ê° {target_min_compound_morph}-{target_max_compound_morph}íšŒ")

        morpheme_instruction_text = " ë° ".join(morpheme_instructions)

        return f"""
        ì´ ë¸”ë¡œê·¸ ì½˜í…ì¸ ë¥¼ ì‚¬ìš©ì ì¹œí™”ì ì´ê³  SEOì— ìµœì í™”ëœ í˜•íƒœë¡œ ê°œì„ í•´ì£¼ì„¸ìš”. ìµœì‹  SEO íŠ¸ë Œë“œì— ë§ì¶° ë‹¤ìŒ ìš”ì†Œë“¤ì— ì§‘ì¤‘í•˜ì„¸ìš”:

        1ï¸âƒ£ ê°€ë…ì„± ìµœì í™”:
        â€¢ ê¸´ ë¬¸ë‹¨ì„ 2-3ë¬¸ì¥ì˜ ì§§ì€ ë¬¸ë‹¨ìœ¼ë¡œ ë¶„ë¦¬
        â€¢ ë³µì¡í•œ ë¬¸ì¥ì„ ê°„ê²°í•˜ê²Œ ì¬êµ¬ì„±
        â€¢ í•µì‹¬ ì •ë³´ëŠ” êµµì€ ê¸€ì”¨ë‚˜ ê°•ì¡° í‘œì‹œ í™œìš©
        â€¢ ëª…í™•í•œ ì†Œì œëª©ìœ¼ë¡œ ì½˜í…ì¸  êµ¬ì¡°í™”
        â€¢ ëª¨ë°”ì¼ì—ì„œ ì½ê¸° ì‰¬ìš´ í˜•ì‹ ì ìš©

        2ï¸âƒ£ í‚¤ì›Œë“œ ë° ì£¼ìš” í˜•íƒœì†Œ ìµœì í™”:
        â€¢ ì£¼ìš” í‚¤ì›Œë“œ '{keyword}'ì™€ ë‹¤ìŒ í˜•íƒœì†Œë“¤ì´ {morpheme_instruction_text} ì¶œí˜„í•˜ë„ë¡ ì¡°ì •í•´ì£¼ì„¸ìš”.
        â€¢ í‚¤ì›Œë“œ ë³€í˜•ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë°°ì¹˜
        â€¢ í‚¤ì›Œë“œ ìŠ¤í„°í•‘(ê³¼ë„í•œ ë°˜ë³µ) ë°©ì§€

        3ï¸âƒ£ êµ¬ì¡°ì  ìµœì í™”:
        â€¢ ì£¼ìš” ì†Œì œëª©(H2, H3)ì— í‚¤ì›Œë“œ í¬í•¨
        â€¢ ì²« ë¬¸ë‹¨ì— í•µì‹¬ í‚¤ì›Œë“œì™€ ì£¼ì œ ëª…í™•íˆ ì œì‹œ
        â€¢ ê¸€ë¨¸ë¦¬ ê¸°í˜¸ì™€ ë²ˆí˜¸ ë§¤ê¸°ê¸°ë¡œ ë‚´ìš© êµ¬ì¡°í™”
        â€¢ ì‹œê°ì  ì—¬ë°±ê³¼ ë¶„ë¦¬ë¥¼ í†µí•œ ì •ë³´ êµ¬ë¶„

        4ï¸âƒ£ ì½˜í…ì¸  í’ˆì§ˆ í–¥ìƒ:
        â€¢ ì „ë¬¸ì ì´ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í†¤ ìœ ì§€
        â€¢ ë¶ˆí•„ìš”í•œ ë°˜ë³µ ì œê±°
        â€¢ í•µì‹¬ ê°€ì¹˜ì™€ ì¤‘ìš” ì •ë³´ ê°•ì¡°
        â€¢ í–‰ë™ ìœ ë„ ë¬¸êµ¬(CTA) ì ì ˆíˆ ë°°ì¹˜

        ì›ë³¸ ì½˜í…ì¸ :
        {content}

        ìµœì í™”ëœ ì½˜í…ì¸ ë§Œ ì œê³µí•´ ì£¼ì„¸ìš”. ì„¤ëª…ì´ë‚˜ ë©”ëª¨ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
        """

    def _create_ultra_seo_prompt(self, content, keyword, custom_morphemes, analysis_result):
        target_min_chars = self.morpheme_analyzer.target_min_chars
        target_max_chars = self.morpheme_analyzer.target_max_chars
        
        base_morphemes = analysis_result['morpheme_analysis']['target_morphemes']['base']
        compound_morphemes = analysis_result['morpheme_analysis']['target_morphemes']['compound']
        current_counts = analysis_result['morpheme_analysis']['counts']

        target_min_base_morph = self.morpheme_analyzer.target_min_base_count
        target_max_base_morph = self.morpheme_analyzer.target_max_base_count
        target_min_compound_morph = self.morpheme_analyzer.target_min_compound_count
        target_max_compound_morph = self.morpheme_analyzer.target_max_compound_count

        morpheme_instructions = []
        if base_morphemes:
            morpheme_instructions.append(f"í•µì‹¬ ê¸°ë³¸ í˜•íƒœì†Œ ({', '.join(base_morphemes)}): ê°ê° {target_min_base_morph}-{target_max_base_morph}íšŒ")
        if compound_morphemes:
            morpheme_instructions.append(f"ë³µí•© í‚¤ì›Œë“œ ë° êµ¬ë¬¸ ({', '.join(compound_morphemes)}): ê°ê° {target_min_compound_morph}-{target_max_compound_morph}íšŒ")

        morpheme_instruction_text = " ë° ".join(morpheme_instructions)

        morpheme_analysis_for_prompt = {
            "target_morphemes": analysis_result['morpheme_analysis']['target_morphemes'],
            "counts": analysis_result['morpheme_analysis']['counts']
        }

        return f"""
        ì´ ë¸”ë¡œê·¸ ê¸€ì„ ì™„ì „í•œ ìµœì í™” ê¸°ì¤€ì— ë§ì¶”ì–´ ì¬êµ¬ì„±í•´ ì£¼ì„¸ìš”. ìµœê³ ì˜ SEO ì„±ëŠ¥ì„ ìœ„í•œ ëª…í™•í•œ ì§€ì¹¨ì„ ë”°ë¼ì£¼ì„¸ìš”:

        1ï¸âƒ£ ì ˆëŒ€ì ì¸ ê¸€ììˆ˜ ìš”êµ¬ì‚¬í•­: 
        â€¢ ìµœì¢… ê¸€ììˆ˜(ê³µë°± ì œì™¸): {target_min_chars}-{target_max_chars}ì ì‚¬ì´ì—¬ì•¼ í•¨
        â€¢ í˜„ì¬ ê¸€ììˆ˜: {analysis_result['char_count']}ì

        2ï¸âƒ£ ì—„ê²©í•œ ëª©í‘œ í˜•íƒœì†Œ ì¶œí˜„ ë¹ˆë„:
        â€¢ ì£¼ìš” í‚¤ì›Œë“œ '{keyword}'ì™€ ì´ì™€ ê´€ë ¨ëœ ì£¼ìš” í˜•íƒœì†Œë“¤({morpheme_instruction_text})ì€ ë°˜ë“œì‹œ ì§€ì •ëœ ë²”ìœ„ ë‚´ë¡œ ì¶œí˜„í•´ì•¼ í•©ë‹ˆë‹¤.
        â€¢ í˜„ì¬ ëª©í‘œ í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼:
        {json.dumps(morpheme_analysis_for_prompt, ensure_ascii=False, indent=2)}

        3ï¸âƒ£ í‚¤ì›Œë“œ ë° í˜•íƒœì†Œ ì¹´ìš´íŒ… ë°©ì‹:
           - í•µì‹¬ ê¸°ë³¸ í˜•íƒœì†Œ (ì˜ˆ: 'ì—”ì§„', 'ì˜¤ì¼', 'ì¢…ë¥˜'): ë¬¸ì¥ ë‚´ì—ì„œ ë¶€ë¶„ì ìœ¼ë¡œ í¬í•¨ë˜ì–´ë„ ì¹´ìš´íŠ¸ë©ë‹ˆë‹¤. (ì˜ˆ: 'ì—”ì§„ì˜¤ì¼ì¢…ë¥˜'ì—ì„œ 'ì—”ì§„' 1íšŒ, 'ì˜¤ì¼' 1íšŒ, 'ì¢…ë¥˜' 1íšŒ)
           - ë³µí•© í‚¤ì›Œë“œ ë° êµ¬ë¬¸ (ì˜ˆ: 'ì—”ì§„ì˜¤ì¼', 'ì—”ì§„ì˜¤ì¼ì¢…ë¥˜'): ì •í™•íˆ í•´ë‹¹ êµ¬ë¬¸ì´ ì¼ì¹˜í•´ì•¼ ì¹´ìš´íŠ¸ë©ë‹ˆë‹¤.

        4ï¸âƒ£ êµ¬ì¡° ìµœì í™” (ì •í™•íˆ ì ìš©):
        â€¢ ì²« ë¬¸ë‹¨ì— ë°˜ë“œì‹œ í‚¤ì›Œë“œì™€ ê·¸ ë³€í˜•ì–´ í¬í•¨
        â€¢ ëª¨ë“  H2/H3 ì œëª©ì— í‚¤ì›Œë“œ ê´€ë ¨ ìš©ì–´ í¬í•¨
        â€¢ 2-3ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¬¸ë‹¨ ë¶„ë¦¬
        â€¢ ì¤‘ìš” ì •ë³´ëŠ” ê¸€ë¨¸ë¦¬ ê¸°í˜¸ë¡œ ê°•ì¡°
        â€¢ ìˆ«ìëŠ” ë¦¬ìŠ¤íŠ¸ë¡œ í‘œì‹œ

        5ï¸âƒ£ ëª¨ë°”ì¼ ìµœì í™”:
        â€¢ 4-5ì¤„ ì´ë‚´ì˜ ì§§ì€ ë¬¸ë‹¨
        â€¢ ë³µì¡í•œ ë¬¸ì¥ ë‹¨ìˆœí™”
        â€¢ ëª¨ë°”ì¼ì—ì„œ ë¹ ë¥´ê²Œ ìŠ¤ìº” ê°€ëŠ¥í•œ í˜•ì‹

        6ï¸âƒ£ í•µì‹¬ ì½˜í…ì¸  êµ¬ì¡°:
        â€¢ ì„œë¡ : í•µì‹¬ í‚¤ì›Œë“œë¡œ ì‹œì‘, ë…ì ë‹ˆì¦ˆ ì–¸ê¸‰
        â€¢ ë³¸ë¡ : ë¬¸ì œì ê³¼ í•´ê²°ì±… ì œì‹œ
        â€¢ ê²°ë¡ : í•µì‹¬ í‚¤ì›Œë“œë¡œ ì •ë¦¬, í–‰ë™ ìœ ë„

        ì›ë³¸ ì½˜í…ì¸ :
        {content}

        ìµœì í™”ëœ ì½˜í…ì¸ ë§Œ ì œê³µí•´ ì£¼ì„¸ìš”. ì„¤ëª…ì´ë‚˜ ë©”ëª¨ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
        """
